<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>multi-armed bandit problem | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="reinforcement learning" />
  
  
  
  
  <meta name="description" content="IntroductionReinforcement Learning Reinforcement Learning are often discribed by Markov Decision Process: Agent is in the Environment, state S is used to discribe the Environment by Agent, and the act">
<meta property="og:type" content="article">
<meta property="og:title" content="Multi-armed Bandit Problem">
<meta property="og:url" content="http://xiangweixi.cn/2021/01/02/Multi-armed-Bandit-Problem/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="IntroductionReinforcement Learning Reinforcement Learning are often discribed by Markov Decision Process: Agent is in the Environment, state S is used to discribe the Environment by Agent, and the act">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2FKmXPKA19gW9QVF1PEPGTibH2emCyu2anPcoAMY3aicP9mJBhw6utOplSia978iacAlDt94MoZ9pTrNh6DCvFibF5bpg%2F0%3Fwx_fmt%3Dpng&refer=http%3A%2F%2Fmmbiz.qpic.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1612169350&t=064fd6caa56f781da10341428e0fe5c6">
<meta property="article:published_time" content="2021-01-02T08:42:04.000Z">
<meta property="article:modified_time" content="2021-01-07T12:30:37.961Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="reinforcement learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2FKmXPKA19gW9QVF1PEPGTibH2emCyu2anPcoAMY3aicP9mJBhw6utOplSia978iacAlDt94MoZ9pTrNh6DCvFibF5bpg%2F0%3Fwx_fmt%3Dpng&refer=http%3A%2F%2Fmmbiz.qpic.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1612169350&t=064fd6caa56f781da10341428e0fe5c6">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Multi-armed-Bandit-Problem" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Multi-armed Bandit Problem
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/01/02/Multi-armed-Bandit-Problem/" class="article-date">
	  <time datetime="2021-01-02T08:42:04.000Z" itemprop="datePublished">2021-01-02</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h3><p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2FKmXPKA19gW9QVF1PEPGTibH2emCyu2anPcoAMY3aicP9mJBhw6utOplSia978iacAlDt94MoZ9pTrNh6DCvFibF5bpg%2F0%3Fwx_fmt%3Dpng&refer=http%3A%2F%2Fmmbiz.qpic.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1612169350&t=064fd6caa56f781da10341428e0fe5c6" alt="&quot;The illustration of reinforcement learning&quot;" title="The illustration of reinforcement learning"></p>
<p>Reinforcement Learning are often discribed by Markov Decision Process: Agent is in the Environment, state S is used to discribe the Environment by Agent, and the actions that Agent can take are A, P is a function that makes E changes its state after Agent takes action with a probability. Environment will gives a reward according to R function when the state transforms. To sum up, E=&lt;X,A,P,R&gt;.</p>
<p>What Agent do is learning a ‚Äúpolicy‚ÄùœÄ by trying over and over agin in the Environment, due to this policy, agent will take action a = œÄ(x) when the state is x.</p>
<p>There are two forms of R function:</p>
<blockquote>
<ul>
<li><p>$$<br>E[\frac{1}{T}\sum_{t=1}^{T}r_{t}]<br>$$</p>
<p>r<sub>t</sub> means reward after step T.</p>
</li>
<li><p>$$<br>E[\sum_{t=0}^{+\infty}\gamma^{t}r_{t+1}]<br>$$</p>
</li>
</ul>
</blockquote>
<h3 id="Multi-Armed-Bandit-Problem"><a href="#Multi-Armed-Bandit-Problem" class="headerlink" title="Multi-Armed Bandit Problem"></a>Multi-Armed Bandit Problem</h3><p>Compared with supervised learning, we don‚Äôt know the results in reinforcement learning in advance. Pretend that you are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your goal is to maximize the expected tatal reward over some time period such as over 1000 action selections.This is the original form of the multi-armed bandit problem.</p>
<p>There is always at least one action whose estimated value is greatest, we call the action the greedy action, if you choose this action, we say you are exploiting. If you choose one of those nongreedy actions, we say you are exploring cause the estimated value of the action you choose will update, and it may be the new greedy action. Our goal is to get a high total reward within finite steps, so it is a question to balance the exploration and exploitation, we call it EE-dilemma.</p>
<h2 id="Action-value-Methods"><a href="#Action-value-Methods" class="headerlink" title="Action-value Methods"></a>Action-value Methods</h2><p>Let us look at methods for estimating the values of actions and using the estimated value to make action selection decision, which we call it action-value methods. We estimate the actions by:<br>$$<br>Q_{t}(a)=\frac{sum\ of\ rewards\ when\ a\ taken\ prior\ of\ t}{number\ of\ times\ a\ taken\ prior\ to\ t}<br>$$<br>If the denominator is 0, then we set Q<sub>t</sub>(a) as a default value like 0. As t goes up, Q<sub>t</sub>(a) converges to q<sub><em></sub>(a) slowly, q<sub>\</em></sub>(a) represents the real estimated reward of the action a without  Q<sub>t</sub>(a) decays everytime you choose action a condition. So the greedy action selection method is:<br>$$<br>A_{t}=<em>{a}^{argmaxQ</em>{t}(a)}<br>$$</p>
<p>Greedy action selection always exploits the action of the maximum estimated reward as known so far, in long run it may not be a good policy, as an alternative, every once in a while, with a small probability ùõÜ, we randomly choose an action among all the actions with equal probability. We call this ùõÜ-greedy policy.</p>
<blockquote>
<p>Exercise: In ùõÜ-greedy action selection, for the case of two actions and ùõÜ = 0.5, what is the probability that the greedy action is selected ?<br>$$<br>P=(1-\epsilon)+\epsilon*\frac{1}{2}=0.75<br>$$</p>
</blockquote>
<h2 id="The-10-armed-Testbed"><a href="#The-10-armed-Testbed" class="headerlink" title="The 10-armed Testbed"></a>The 10-armed Testbed</h2><p>For any learning method, we can measure its performance as it improves with experience over 1000 time steps when applied to 10-armed bandit problem.This makes up one run, repeating it for 2000 independent runs each with a different 10-armed bandit problem, we can get the average performance of the learning algorithm. We call this method the 10-armed testbed.</p>
<p>Whether or not use ùõÜ-greedy method depends on the reward variance of the task, the larger it is, the more we need to use ùõÜ-greedy method. As the nonstationary case is the most commonly encountered in reinforcement learning, out agents do need to make a balance between exploration and exploitation.</p>
<h2 id="Incremental-Implementation"><a href="#Incremental-Implementation" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h2><p>To simplify the equation, let R<sub>t</sub> now denote the reward received after the i step, and let Q<sub>n</sub> denote the estimate of its action value after selected n-1 times, we can now express the equation as<br>$$<br>Q_{n}=\frac{R_{1}+R_{2}+‚Ä¶+R_{n-1}}{n-1}<br>$$<br>Given Q<sub>n</sub> and the nth reward, R<sub>n</sub>, the new average of all n rewards can be computed by<br>$$<br>Q_{n+1}=\frac{1}{n}\sum_{i=1}^{n}{R_{i}}\\=\frac{1}{n}(R_{n}+\sum_{i=1}^{n-1}{R_{i}})\\=\frac{1}{n}(R_{n}+(n-1)\sum_{i=1}^{n-1}R_{i})\\=\frac{1}{n}(R_{n}+(n-1)Q_{n})\\=\frac{1}{n}(R_{n}+nQ_{n}-Q_{n})\\=Q_{n}+\frac{1}{n}(R_{n}-Q_{n})<br>$$<br>It is of a form that occurs frequently in reinforcement learning, the general form is<br>$$<br>NewEstimate&lt;-OldEstimate+StepSize[Target-OldEstimate]<br>$$<br>We call [Target-OldEstimate] error, besides, note that the step-size parameter used in the incremental method changes from time step to time step. We denote the step-size parameter by ùù∞ or, more generally, by ùù∞<sub>t</sub>(a).</p>
<h2 id="Tracking-a-Nonstationary-Problem"><a href="#Tracking-a-Nonstationary-Problem" class="headerlink" title="Tracking a Nonstationary Problem"></a>Tracking a Nonstationary Problem</h2><p>In the nonstationary case, we should give more weight to recent rewards than to long-past rewards. A way of doing this is to use a constant step-size parameter. We modify the updating rule to:<br>$$<br>Q_{n+1}=Q_{n}+\alpha[R_{n}-Q_{n}]<br>$$<br>ùù∞‚àà(0,1], the results in Q<sub>n+1</sub> can be presented by Q<sub>1</sub> and R<sub>i</sub>:<br>$$<br>Q_{n+1}=Q_{n}+\alpha[R_{n}-Q_{n}]\\=\alpha R_{n}+(1-\alpha)Q_{n}\\=\alpha R_{n}+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]\\=\alpha R_{n}+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^{2}Q_{n-1}\=‚Ä¶\=(1-\alpha)^{n}Q_{1}+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-i}R_{i}<br>$$<br>It is called a weighted average cause the sum of the weights is 1, the weight of R<sub>i</sub> decays because ùù∞‚àà(0,1]. Accordingly, we sometimes call this an exponential recency-weighted average.</p>
<p>We can vary the step-size parameter from step to step, let ùù∞<sub>n</sub>(a) denote the step-size parameter used to process the reward after nth selection of action a. Then ùù∞<sub>n</sub>(a) have properties:<br>$$<br>\sum_{n=1}^{‚àû}{\alpha_{n}(a)}=‚àû<br>$$</p>
<p>$$<br>\sum_{n=1}^{‚àû}{\alpha_{n}^2(a)}&lt;‚àû<br>$$</p>
<p>The first condition is to overcome any initial conditons or random fluctuations by giving an enough large steps, and the second condition is to guarantee eventually the steps are small enough to converge. Thus the sample-average step-size parameter meets both convergence conditions, but the weighted average sample method doesn‚Äôt. However, sequence of step-size parameters meet the conditions above often converges very slowly without considerable tuning to get a proper convergence rate, so we only use them in theoretical work most of the time.</p>
<h2 id="Optimistic-Initial-values"><a href="#Optimistic-Initial-values" class="headerlink" title="Optimistic Initial values"></a>Optimistic Initial values</h2><p>The above methods dependent on its initial action-value estimates, Q<sub>1</sub>(a), In statistics, we  say these methods are biased by their initial estimates. For the sample-average methods, the bias disappears when all the actions have been taken at least once, but for weighted average sample methods, the bias won‚Äôt disappear, even if it decreases over time.</p>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/01/02/Multi-armed-Bandit-Problem/" target="_blank" title="Multi-armed Bandit Problem">http://xiangweixi.cn/2021/01/02/Multi-armed-Bandit-Problem/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- Êù•ÂøÖÂäõCityÁâàÂÆâË£Ö‰ª£Á†Å -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>‰∏∫Ê≠£Â∏∏‰ΩøÁî®Êù•ÂøÖÂäõËØÑËÆ∫ÂäüËÉΩËØ∑ÊøÄÊ¥ªJavaScript</noscript>
		</div>
		<!-- CityÁâàÂÆâË£Ö‰ª£Á†ÅÂ∑≤ÂÆåÊàê -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/reinforcement-learning/" rel="tag">reinforcement learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/01/03/%E5%8E%BB2021/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Âéª2021
        
      </div>
    </a>
  
  
    <a href="/2021/01/02/LightGBM-born-to-win/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">LightGBM:born to win</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reinforcement-Learning"><span class="nav-number">1.1.</span> <span class="nav-text">Reinforcement Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Armed-Bandit-Problem"><span class="nav-number">1.2.</span> <span class="nav-text">Multi-Armed Bandit Problem</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Action-value-Methods"><span class="nav-number">2.</span> <span class="nav-text">Action-value Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-10-armed-Testbed"><span class="nav-number">3.</span> <span class="nav-text">The 10-armed Testbed</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Incremental-Implementation"><span class="nav-number">4.</span> <span class="nav-text">Incremental Implementation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tracking-a-Nonstationary-Problem"><span class="nav-number">5.</span> <span class="nav-text">Tracking a Nonstationary Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimistic-Initial-values"><span class="nav-number">6.</span> <span class="nav-text">Optimistic Initial values</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">ËÆæÁΩÆ</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              Ê≠£ÊñáÂ≠óÂè∑Â§ßÂ∞è
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            ÊÇ®Â∑≤Ë∞ÉÊï¥È°µÈù¢Â≠ó‰ΩìÂ§ßÂ∞è
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              Â§úÈó¥Êä§ÁúºÊ®°Âºè
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            Â§úÈó¥Ê®°ÂºèÂ∑≤ÁªèÂºÄÂêØÔºåÂÜçÊ¨°ÂçïÂáªÊåâÈíÆÂç≥ÂèØÂÖ≥Èó≠ 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ÂÖ≥ ‰∫é&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright ¬© 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">√ó</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>