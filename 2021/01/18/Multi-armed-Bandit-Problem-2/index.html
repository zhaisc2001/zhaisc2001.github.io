<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>multi-armed-bandit-problem(2) | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="reinforcement learning" />
  
  
  
  
  <meta name="description" content="ExerciseAt first, let‚Äôs do the exercise:   If the step-size parameters, ùù∞n, are not constant, then the estimate Qn is  a weighted average of previously received rewards with a weighting di‚Üµerent from">
<meta property="og:type" content="article">
<meta property="og:title" content="Multi-armed-Bandit-Problem(2)">
<meta property="og:url" content="http://xiangweixi.cn/2021/01/18/Multi-armed-Bandit-Problem-2/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="ExerciseAt first, let‚Äôs do the exercise:   If the step-size parameters, ùù∞n, are not constant, then the estimate Qn is  a weighted average of previously received rewards with a weighting di‚Üµerent from">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-01-18T06:12:37.000Z">
<meta property="article:modified_time" content="2021-01-23T01:24:46.125Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="reinforcement learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Multi-armed-Bandit-Problem-2" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Multi-armed-Bandit-Problem(2)
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/01/18/Multi-armed-Bandit-Problem-2/" class="article-date">
	  <time datetime="2021-01-18T06:12:37.000Z" itemprop="datePublished">2021-01-18</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h1><p>At first, let‚Äôs do the exercise:</p>
<blockquote>
<ul>
<li>If the step-size parameters, ùù∞<sub>n</sub>, are not constant, then the estimate Q<sub>n</sub> is  a weighted average of previously received rewards with a weighting di‚Üµerent from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?</li>
</ul>
</blockquote>
<p>For the question, we have:<br>$$<br>Q_{n}=Q_{n-1}+\alpha_{n}(R_{n}-Q_{n-1})\=Q_{n-1}+\alpha_{n}R_{n}-\alpha_{n}Q_{n-1}<br>\=\alpha_{n}R_{n}+(1-\alpha_{n})[Q_{n-2}+\alpha_{n-1}(R_{n-1}-Q_{n-2})]\=‚Ä¶=\alpha_{n}R_{n}+\sum_{i=1}^{n-1}\prod_{t=i+1}^{n}(1-\alpha_{t})\alpha_{i}R_{i}+\prod_{i=1}^{n}(1-\alpha_{i})Q_{1}<br>$$</p>
<h1 id="Optimistic-Initial-Values"><a href="#Optimistic-Initial-Values" class="headerlink" title="Optimistic Initial Values"></a>Optimistic Initial Values</h1><p>If the methods are dependent on the initial acton-value estimates Q<sub>1</sub>(a) to some extent, we say these methods are biased by their initial estimates in the language of statistics. For the sample-average methods, the bias will disappear once all actions have been selected once, but for methods with step-size parameter ùù∞, the bias is permanent even though it will decrease as time goes by. The upside of the bias is that they provide an easy way to supply some prior knowledge about what level of rewards can be expected, on the other hand, the downside is that the initial estimates must be picked by the user, if only to set them all to 0.</p>
<p>Initial action values can be used as a simple way to encourage exploration, if we set the initial value as +5 and the q<sub>*</sub>(a) in the problem are selected from a norml distribution with mean 0 and variance 1. An initial estimate of +5 is thus widly optimistic, it will encourage the action-value methods to explore cause whichever actions are selected, the reward is less than the starting estimate, we call this technique for encouraging exploration optimistic initial values. It‚Äôs just a simple trick can be quite effective on stationary problems, but it‚Äôs far from being a general approach to encouraging exploration.</p>
<p>Exercise:</p>
<blockquote>
<ul>
<li><p>Unbiased Constant-Step-Size Trick In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of constant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of<br>$$<br>\beta_{n}=\alpha/o_{n},<br>$$<br>to process the nth reward for a particular action, where ùù∞ &gt; 0 is a conventional constant</p>
<p>step size, and o<sub>n</sub> is a trace of one that starts at 0:<br>$$<br>o_{n}=o_{n-1}+\alpha(1-o_{n-1}),for\ n‚â•0,with\ o_{0}=0.<br>$$<br>Carry out an analysis to show that Qn is an exponential recency-weighted average without initial bias.</p>
</li>
</ul>
</blockquote>
<p>We have:<br>$$<br>Q_{n}=Q_{n-1}+\beta_{n}(R_{n}-Q_{n-1})\<br>=\beta_{n}R_{n}+\sum_{i=0}^{n-1}\prod_{t=i+1}^{n}(1-\beta_{t})\beta_{i}R_{i}+\prod_{i=0}^{n}(1-\beta_{i})Q_{0}\<br>=\beta_{n}R_{n}+\sum_{i=0}^{n-1}\prod_{t=i+1}^{n}(1-\beta_{t})\beta_{i}R_{i}+\frac{(1-\alpha)o_{n-1}}{o_{n}}\frac{(1-\alpha)o_{n-2}}{o_{n-1}}‚Ä¶\frac{(1-\alpha)o_{0}}{o_{1}}Q_{0}\=\beta_{n}R_{n}+\sum_{i=0}^{n-1}\prod_{t=i+1}^{n}(1-\beta_{t})\beta_{i}R_{i}+\frac{(1-\alpha)^no_{0}}{o_{n}}Q_{0}\because\ o_{0}=0,so\ Q_{n}=\beta_{n}R_{n}+\sum_{i=0}^{n-1}\prod_{t=i+1}^{n}(1-\beta_{t})\beta_{i}R_{i}<br>$$</p>
<h1 id="Upper-Confidence-Bound-Action-Selection"><a href="#Upper-Confidence-Bound-Action-Selection" class="headerlink" title="Upper-Confidence-Bound Action Selection"></a>Upper-Confidence-Bound Action Selection</h1><p>The greedy actions look best at present, but there may be some of the other acitons actually be better. ùõÜ-greedy action selection forces the non-greedy acitons to be tired indiscriminately. We should choose those non-greedy actions which are nearly greedy or particularly uncertain, one effective way of doing this is to select actions according to<br>$$<br>A_{t}=argmax_{a}[Q_{t}(a)+c\sqrt{\frac{lnt}{N_{t}(a)}}]<br>$$<br>N<sub>t</sub>(a) denotes the number of times that action a has been selected prior to time t, if N<sub>t</sub> = 0, then a is considered to be a maximizing action. The idea of UCB is that the square-root term is a measure of the variance in the estimate of a‚Äôs value. Each time action a is selected, N<sub>t</sub>(a) increases and A<sub>t</sub> decreases, each time an action other than a is selected, t increases but N<sub>t</sub> doesn‚Äôt increase, so A<sub>t</sub> increases. The use of natural logarithm means that the increases get smaller but are unbounded.</p>
<h1 id="Gradient-Bandit-Algorithms"><a href="#Gradient-Bandit-Algorithms" class="headerlink" title="Gradient Bandit Algorithms"></a>Gradient Bandit Algorithms</h1><p>Let‚Äôs consider a numerical preference for each action a, which we denote H<sub>t</sub>(a). The preference is not reward, only the relative preference of one action over another is important, they are determined as follows:<br>$$<br>Pr[A_{t}=a]=\frac{e^{H_{t}(a)}}{\sum_{b=1}^{k}e^{H_{t}(b)}}=\pi_{t}(a)<br>$$</p>
<p>We call it a soft-max distribution(i.e., Gibbs or Boltzmann distribution). Besides, œÄ<sub>t</sub>(a) means the probability of taking action a at time t. All action preferences are the same in the start.</p>
<p>Exercise:</p>
<blockquote>
<ul>
<li>Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.</li>
</ul>
</blockquote>
<p>Use the sigmoid function, we have:<br>$$<br>Pr[A_{t}=a]=\frac{e^{H_{t}(a)}}{1+e^{H_{t}(a)}},Pr[A_{t}=b]=1-Pr[A_t=a]=\frac{1}{1+e^{H_{t}(a)}}<br>$$<br>Use the Soft-max distribution, we have:<br>$$<br>Pr[A_{t}=a]=\frac{e^{H_{t}(a)}}{e^{H_{t}(a)}+e^{H_{t}(b)}},Pr[A_t=b]=\frac{e^{H_{t}(b)}}{e^{H_{t}(a)}+e^{H_{t}(b)}}<br>$$<br>From the definition of Preference, we know that it does not affect the probabilities if both preferences minus the same number. So we redefine H<sub>t</sub>(a) and H<sub>t</sub>(b) as:<br>$$<br>H_{t}(b)=H_{t}(b)-H_{t}(b)=0;H_{t}(a)=H_{t}(a)-H_{t}(b)=H_{t}(a)\<br>So\ Pr[A_{t}=a]=\frac{e^{H_{t}(a)}}{e^{H_{t}(a)}+e^{0}}=\frac{e^{H_{t}(a)}}{1+e^{H_{t}(a)}},Pr[A_{t}=b]=\frac{e^{0}}{e^{H_{t}(a)}+e^{0}}=\frac{1}{1+e^{H_{t}(a)}}<br>$$<br>We update the preferences by the learning algorithm based on the idea of stochastic gradient ascent:<br>$$<br>H_{t+1}(A_{t})=H_{t}(A_{t})+\alpha(R_{t}-\overline{R_{t}})(1-\pi_{t}(A_{t})), and\<br>H_{t+1}(a)=H_{t}(a)-\alpha(R_{t}-\overline{R_{t}})\pi_{t}(A_{t}),for\ all\ a‚â†A_{t}<br>$$<br>ùù∞&gt;0 is a step-size parameter, R with overline is the average reward of action A<sub>t</sub> before step t.</p>
<h1 id="Associative-Search-Contextual-Bandits"><a href="#Associative-Search-Contextual-Bandits" class="headerlink" title="Associative Search(Contextual Bandits)"></a>Associative Search(Contextual Bandits)</h1><p>So far our tasks are either trying to find a single best action when the task is stationary, or trying to track the best action as it changes over time when the task is nonstationary. But in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy which is a mapping from situations to the actions that are best in those situations. These are nonassociative tasks, let‚Äôs talk about associative tasks.</p>
<p>As an example, suppose there is a nonstationary k-armed bandit tasks, and each step you will confront a random one of them. If the probabilities with which bandit you choose do not change over time, this would become a single stationary bandit task, and you can find the best bandit simply. Now, let‚Äôs suppose that when you select a bandit, it won‚Äôt give you action value but other clue. For example, when the bandit changes its action value, it will change its color. In this case, you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. And that‚Äôs an example of associative search.</p>
<p>Exercise:</p>
<blockquote>
<ul>
<li>Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case A), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expected reward you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don‚Äôt know the true action values). This is an associative search task. What is the best expected reward you can achieve in this task, and how should you behave to achieve it?</li>
</ul>
</blockquote>
<p>In case A, action 1 and action 2 both have an expected reward 50, so it doen‚Äôt matter how you behave.</p>
<p>In case B, because you are told whether you are facing case A or case B, so you should run a normal bandit method and make the optimal choice, the expected reward of optimal choice is 55.</p>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/01/18/Multi-armed-Bandit-Problem-2/" target="_blank" title="Multi-armed-Bandit-Problem(2)">http://xiangweixi.cn/2021/01/18/Multi-armed-Bandit-Problem-2/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- Êù•ÂøÖÂäõCityÁâàÂÆâË£Ö‰ª£Á†Å -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>‰∏∫Ê≠£Â∏∏‰ΩøÁî®Êù•ÂøÖÂäõËØÑËÆ∫ÂäüËÉΩËØ∑ÊøÄÊ¥ªJavaScript</noscript>
		</div>
		<!-- CityÁâàÂÆâË£Ö‰ª£Á†ÅÂ∑≤ÂÆåÊàê -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/reinforcement-learning/" rel="tag">reinforcement learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/01/19/%E6%9C%88%E8%AE%A1%E5%88%92-1%E6%9C%8816%E6%97%A5-2%E6%9C%8824%E6%97%A5/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          ÊúàËÆ°Âàí(1Êúà16Êó•-2Êúà24Êó•)
        
      </div>
    </a>
  
  
    <a href="/2021/01/10/Cerebral-microdialysis-combined-with-single-neuron-and-electroencephalographic-recording-in-neurosurgical-patients%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Cerebral microdialysis combined with single-neuron and electroencephalographic recording in neurosurgical patientsÈòÖËØªÁ¨îËÆ∞</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Exercise"><span class="nav-number">1.</span> <span class="nav-text">Exercise</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Optimistic-Initial-Values"><span class="nav-number">2.</span> <span class="nav-text">Optimistic Initial Values</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Upper-Confidence-Bound-Action-Selection"><span class="nav-number">3.</span> <span class="nav-text">Upper-Confidence-Bound Action Selection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gradient-Bandit-Algorithms"><span class="nav-number">4.</span> <span class="nav-text">Gradient Bandit Algorithms</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Associative-Search-Contextual-Bandits"><span class="nav-number">5.</span> <span class="nav-text">Associative Search(Contextual Bandits)</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">ËÆæÁΩÆ</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              Ê≠£ÊñáÂ≠óÂè∑Â§ßÂ∞è
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            ÊÇ®Â∑≤Ë∞ÉÊï¥È°µÈù¢Â≠ó‰ΩìÂ§ßÂ∞è
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              Â§úÈó¥Êä§ÁúºÊ®°Âºè
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            Â§úÈó¥Ê®°ÂºèÂ∑≤ÁªèÂºÄÂêØÔºåÂÜçÊ¨°ÂçïÂáªÊåâÈíÆÂç≥ÂèØÂÖ≥Èó≠ 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ÂÖ≥ ‰∫é&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright ¬© 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">√ó</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>