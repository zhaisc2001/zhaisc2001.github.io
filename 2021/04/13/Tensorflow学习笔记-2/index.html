<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>tensorflow学习笔记-2 | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="tensorflow" />
  
  
  
  
  <meta name="description" content="ImageCNNThis tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training our model w">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow学习笔记-2">
<meta property="og:url" content="http://xiangweixi.cn/2021/04/13/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="ImageCNNThis tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training our model w">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tensorflow.google.cn/tutorials/images/segmentation_files/output_sw82qF1Gcovr_0.png">
<meta property="article:published_time" content="2021-04-13T06:37:29.000Z">
<meta property="article:modified_time" content="2021-04-14T14:15:32.073Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tensorflow.google.cn/tutorials/images/segmentation_files/output_sw82qF1Gcovr_0.png">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form" method="GET" action="https://www.baidu.com/s?">
    <input name="wd" type="text" class="search-form-input" placeholder="index.search" />
    <button type="submit" class="search-form-submit"></button>
</form>
<script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:xiangweixi.cn ' + keyword;
        return false;
    });
})(jQuery);
</script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Tensorflow学习笔记-2" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Tensorflow学习笔记-2
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/04/13/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/" class="article-date">
	  <time datetime="2021-04-13T06:37:29.000Z" itemprop="datePublished">2021-04-13</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>This tutorial demonstrates training a simple <a target="_blank" rel="noopener" href="https://developers.google.cn/machine-learning/glossary/#convolutional_neural_network">Convolutional Neural Network</a> (CNN) to classify <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR images</a>. Because this tutorial uses the <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/keras/overview">Keras Sequential API</a>, creating and training our model will take just a few lines of code.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                                                    activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                                                    input_shape=(</span><br><span class="line">                                                        <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)),</span><br><span class="line">                             tf.keras.layers.MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">                             tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>),</span><br><span class="line">                                                    activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                             tf.keras.layers.MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">                             tf.keras.layers.Conv2D(<span class="number">64</span>,(<span class="number">3</span>,<span class="number">3</span>),</span><br><span class="line">                                                    activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                             tf.keras.layers.Flatten(),</span><br><span class="line">                             tf.keras.layers.Dense(<span class="number">64</span>,activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                             tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">                              ])</span><br><span class="line"></span><br><span class="line">print(my_seq.summary())</span><br><span class="line"></span><br><span class="line">my_seq.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h3 id="Configure-the-dataset-for-performance"><a href="#Configure-the-dataset-for-performance" class="headerlink" title="Configure the dataset for performance"></a>Configure the dataset for performance</h3><p>Let’s make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.</p>
<p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#cache"><code>Dataset.cache()</code></a> keeps the images in memory after they’re loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.</p>
<p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#prefetch"><code>Dataset.prefetch()</code></a> overlaps data preprocessing and model execution while training.</p>
<p>Interested readers can learn more about both methods, as well as how to cache data to disk in the <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/data_performance#prefetching">data performance guide</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">AUTOTUNE = tf.data.AUTOTUNE</span><br><span class="line"></span><br><span class="line">train_ds = train_ds.cache().shuffle(<span class="number">1000</span>).prefetch(buffer_size=AUTOTUNE)</span><br><span class="line">val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h3><p>Overfitting generally occurs when there are a small number of training examples. <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/tutorials/images/data_augmentation">Data augmentation</a> takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.</p>
<p>You will implement data augmentation using the layers from <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing"><code>tf.keras.layers.experimental.preprocessing</code></a>. These can be included inside your model like other layers, and run on the GPU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_augmentation = keras.Sequential(</span><br><span class="line">  [</span><br><span class="line">    layers.experimental.preprocessing.RandomFlip(<span class="string">&quot;horizontal&quot;</span>, </span><br><span class="line">                                                 input_shape=(img_height, </span><br><span class="line">                                                              img_width,</span><br><span class="line">                                                              <span class="number">3</span>)),</span><br><span class="line">    layers.experimental.preprocessing.RandomRotation(<span class="number">0.1</span>),</span><br><span class="line">    layers.experimental.preprocessing.RandomZoom(<span class="number">0.1</span>),</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">resize_and_rescalue = keras.Sequential([</span><br><span class="line">    layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),</span><br><span class="line">  	layers.experimental.preprocessing.Rescaling(<span class="number">1.</span>/<span class="number">255</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="Option-1-Make-the-preprocessing-layers-part-of-your-model"><a href="#Option-1-Make-the-preprocessing-layers-part-of-your-model" class="headerlink" title="Option 1: Make the preprocessing layers part of your model"></a>Option 1: Make the preprocessing layers part of your model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">  resize_and_rescale,</span><br><span class="line">  data_augmentation,</span><br><span class="line">  layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  layers.MaxPooling2D(),</span><br><span class="line">  <span class="comment"># Rest of your model</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="Option-2-Apply-the-preprocessing-layers-to-your-dataset"><a href="#Option-2-Apply-the-preprocessing-layers-to-your-dataset" class="headerlink" title="Option 2: Apply the preprocessing layers to your dataset"></a>Option 2: Apply the preprocessing layers to your dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">aug_ds = train_ds.<span class="built_in">map</span>(</span><br><span class="line">  <span class="keyword">lambda</span> x, y: (resize_and_rescale(x, training=<span class="literal">True</span>), y))</span><br></pre></td></tr></table></figure>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Another technique to reduce overfitting is to introduce <a target="_blank" rel="noopener" href="https://developers.google.cn/machine-learning/glossary#dropout_regularization">Dropout</a> to the network, a form of <em>regularization</em>.</p>
<p>When you apply Dropout to a layer it randomly drops out (by setting the activation to zero) a number of output units from the layer during the training process. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer.</p>
<p>Let’s create a new neural network using <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Dropout"><code>layers.Dropout</code></a>, then train it using augmented images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">  data_augmentation,</span><br><span class="line">  layers.experimental.preprocessing.Rescaling(<span class="number">1.</span>/<span class="number">255</span>),</span><br><span class="line">  layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  layers.MaxPooling2D(),</span><br><span class="line">  layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  layers.MaxPooling2D(),</span><br><span class="line">  layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  layers.MaxPooling2D(),</span><br><span class="line">  layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">  layers.Flatten(),</span><br><span class="line">  layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  layers.Dense(num_classes)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h3><p>A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.</p>
<p>The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.</p>
<p>In this notebook, you will try two ways to customize a pretrained model:</p>
<ol>
<li><p>Feature Extraction: Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.</p>
<p>You do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.</p>
</li>
<li><p>Fine-Tuning: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to “fine-tune” the higher-order feature representations in the base model in order to make them more relevant for the specific task.</p>
</li>
</ol>
<p>You will create the base model from the <strong>MobileNet V2</strong> model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like <code>jackfruit</code> and <code>syringe</code>. This base of knowledge will help us classify cats and dogs from our specific dataset.</p>
<p>First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on “top”, as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the “bottleneck layer”. The bottleneck layer features retain more generality as compared to the final/top layer.</p>
<p>First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the <strong>include_top=False</strong> argument, you load a network that doesn’t include the classification layers at the top, which is ideal for feature extraction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> image_dataset_from_directory</span><br><span class="line"></span><br><span class="line">base_model = tf.keras.applications.MobileNetV2(input_shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>),</span><br><span class="line">                                               include_top=<span class="literal">False</span>,</span><br><span class="line">                                               weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="feature-extraction"><a href="#feature-extraction" class="headerlink" title="feature extraction"></a>feature extraction</h3><p>In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">base_model.trainable = <span class="literal">False</span></span><br><span class="line">base_model.summary()</span><br></pre></td></tr></table></figure>
<p>To generate predictions from the block of features, average over the spatial <code>5x5</code> spatial locations, using a <code>tf.keras.layers.GlobalAveragePooling2D</code> layer to convert the features to a single 1280-element vector per image.</p>
<p>Apply a <code>tf.keras.layers.Dense</code> layer to convert these features into a single prediction per image. You don’t need an activation function here because this prediction will be treated as a <code>logit</code>, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0.</p>
<p>Build a model by chaining together the data augmentation, rescaling, base_model and feature extractor layers using the <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/keras/functional">Keras Functional API</a>. As previously mentioned, use training=False as our model contains a BatchNormalization layer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> image_dataset_from_directory</span><br><span class="line"></span><br><span class="line">data_augmentation = tf.keras.Sequential([</span><br><span class="line">  tf.keras.layers.experimental.preprocessing.RandomFlip(<span class="string">&#x27;horizontal&#x27;</span>),</span><br><span class="line">  tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="number">0.2</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input</span><br><span class="line"></span><br><span class="line">base_model = tf.keras.applications.MobileNetV2(input_shape=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">3</span>),</span><br><span class="line">                                               include_top=<span class="literal">False</span>,</span><br><span class="line">                                               weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line">base_model.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">global_average_layer = tf.keras.layers.GlobalAveragePooling2D()</span><br><span class="line"></span><br><span class="line">prediction_layer = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">inputs = tf.keras.Input(shape=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">3</span>))</span><br><span class="line">x = data_augmentation(inputs)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line">x = base_model(x, training=<span class="literal">False</span>)</span><br><span class="line">x = global_average_layer(x)</span><br><span class="line">x = tf.keras.layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">outputs = prediction_layer(x)</span><br><span class="line">model = tf.keras.Model(inputs, outputs)</span><br><span class="line"></span><br><span class="line">base_learning_rate = <span class="number">0.0001</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),</span><br><span class="line">              loss=tf.keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h3 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine tuning"></a>fine tuning</h3><p>n the feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were <strong>not</strong> updated during training.</p>
<p>One way to increase performance even further is to train (or “fine-tune”) the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.</p>
<p>Also, you should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">base_model.trainable = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">fine_tune_at = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> base_model.layers[:fine_tune_at]:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>Using a pre-trained model for feature extraction</strong>: When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is “frozen” and only the weights of the classifier get updated during training. In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.</p>
</li>
<li><p><strong>Fine-tuning a pre-trained model</strong>: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.</p>
</li>
</ul>
<h3 id="Custom-data-augmentation"><a href="#Custom-data-augmentation" class="headerlink" title="Custom data augmentation"></a>Custom data augmentation</h3><p>You can also create custom data augmenation layers. This tutorial shows two ways of doing so. First, you will create a <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Lambda"><code>layers.Lambda</code></a> layer. This is a good way to write concise code. Next, you will write a new layer via <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/keras/custom_layers_and_models">subclassing</a>, which gives you more control. Both layers will randomly invert the colors in an image, according to some probability.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_invert_img</span>(<span class="params">x, p=<span class="number">0.5</span></span>):</span></span><br><span class="line">  <span class="keyword">if</span>  tf.random.uniform([]) &lt; p:</span><br><span class="line">    x = (<span class="number">255</span>-x)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    x</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_invert</span>(<span class="params">factor=<span class="number">0.5</span></span>):</span></span><br><span class="line">  <span class="keyword">return</span> layers.Lambda(<span class="keyword">lambda</span> x: random_invert_img(x, factor))</span><br><span class="line"></span><br><span class="line">random_invert = random_invert()</span><br></pre></td></tr></table></figure>
<h3 id="Apply-augmentation-to-a-dataset"><a href="#Apply-augmentation-to-a-dataset" class="headerlink" title="Apply augmentation to a dataset"></a>Apply augmentation to a dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resize_and_rescale</span>(<span class="params">image, label</span>):</span></span><br><span class="line">  image = tf.cast(image, tf.float32)</span><br><span class="line">  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])</span><br><span class="line">  image = (image / <span class="number">255.0</span>)</span><br><span class="line">  <span class="keyword">return</span> image, label</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">augment</span>(<span class="params">image_label, seed</span>):</span></span><br><span class="line">  image, label = image_label</span><br><span class="line">  image, label = resize_and_rescale(image, label)</span><br><span class="line">  image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + <span class="number">6</span>, IMG_SIZE + <span class="number">6</span>)</span><br><span class="line">  <span class="comment"># Make a new seed</span></span><br><span class="line">  new_seed = tf.random.experimental.stateless_split(seed, num=<span class="number">1</span>)[<span class="number">0</span>, :]</span><br><span class="line">  <span class="comment"># Random crop back to the original size</span></span><br><span class="line">  image = tf.image.stateless_random_crop(</span><br><span class="line">      image, size=[IMG_SIZE, IMG_SIZE, <span class="number">3</span>], seed=seed)</span><br><span class="line">  <span class="comment"># Random brightness</span></span><br><span class="line">  image = tf.image.stateless_random_brightness(</span><br><span class="line">      image, max_delta=<span class="number">0.5</span>, seed=new_seed)</span><br><span class="line">  image = tf.clip_by_value(image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>
<h4 id="Option-1-Using-tf-data-experimental-Counter"><a href="#Option-1-Using-tf-data-experimental-Counter" class="headerlink" title="Option 1: Using tf.data.experimental.Counter()"></a>Option 1: Using <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/experimental/Counter"><code>tf.data.experimental.Counter()</code></a></h4><p>Create a <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/experimental/Counter"><code>tf.data.experimental.Counter()</code></a> object (let’s call it <code>counter</code>) and <code>zip</code> the dataset with <code>(counter, counter)</code>. This will ensure that each image in the dataset gets associated with a unique value (of shape <code>(2,)</code>) based on <code>counter</code> which later can get passed into the <code>augment</code> function as the <code>seed</code> value for random transformations.</p>
<p>Map the <code>augment</code> function to the training dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_ds = (</span><br><span class="line">    train_ds</span><br><span class="line">    .shuffle(<span class="number">1000</span>)</span><br><span class="line">    .<span class="built_in">map</span>(augment, num_parallel_calls=AUTOTUNE)</span><br><span class="line">    .batch(batch_size)</span><br><span class="line">    .prefetch(AUTOTUNE)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="Option-2-Using-tf-random-Generator"><a href="#Option-2-Using-tf-random-Generator" class="headerlink" title="Option 2: Using tf.random.Generator"></a>Option 2: Using <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/random/Generator"><code>tf.random.Generator</code></a></h4><p>Create a <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/random/Generator"><code>tf.random.Generator</code></a> object with an intial <code>seed</code> value. Calling <code>make_seeds</code> function on the same generator object returns a new, unique <code>seed</code> value always. Define a wrapper function that 1) calls <code>make_seeds</code> function and that 2) passes the newly generated <code>seed</code> value into the <code>augment</code> function for random transformations.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rng = tf.random.Generator.from_seed(<span class="number">123</span>, alg=<span class="string">&#x27;philox&#x27;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  seed = rng.make_seeds(<span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">  image, label = augment((x, y), seed)</span><br><span class="line">  <span class="keyword">return</span> image, label</span><br><span class="line">train_ds = (</span><br><span class="line">    train_datasets</span><br><span class="line">    .shuffle(<span class="number">1000</span>)</span><br><span class="line">    .<span class="built_in">map</span>(f, num_parallel_calls=AUTOTUNE)</span><br><span class="line">    .batch(batch_size)</span><br><span class="line">    .prefetch(AUTOTUNE)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="image-segmentation"><a href="#image-segmentation" class="headerlink" title="image segmentation"></a>image segmentation</h3><p>So far you have seen image classification, where the task of the network is to assign a label or class to an input image. However, suppose you want to know where an object is located in the image, the shape of that object, which pixel belongs to which object, etc. In this case you will want to segment the image, i.e., each pixel of the image is given a label. Thus, the task of image segmentation is to train a neural network to output a pixel-wise mask of the image. This helps in understanding the image at a much lower level, i.e., the pixel level. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging to name a few.</p>
<p>The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the <a target="_blank" rel="noopener" href="https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py">Pix2pix tutorial</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow_examples.models.pix2pix <span class="keyword">import</span> pix2pix</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">base_model = tf.keras.applications.MobileNetV2(input_shape=[<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>], include_top=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the activations of these layers</span></span><br><span class="line">layer_names = [</span><br><span class="line">    <span class="string">&#x27;block_1_expand_relu&#x27;</span>,   <span class="comment"># 64x64</span></span><br><span class="line">    <span class="string">&#x27;block_3_expand_relu&#x27;</span>,   <span class="comment"># 32x32</span></span><br><span class="line">    <span class="string">&#x27;block_6_expand_relu&#x27;</span>,   <span class="comment"># 16x16</span></span><br><span class="line">    <span class="string">&#x27;block_13_expand_relu&#x27;</span>,  <span class="comment"># 8x8</span></span><br><span class="line">    <span class="string">&#x27;block_16_project&#x27;</span>,      <span class="comment"># 4x4</span></span><br><span class="line">]</span><br><span class="line">base_model_outputs = [base_model.get_layer(name).output <span class="keyword">for</span> name <span class="keyword">in</span> layer_names]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the feature extraction model</span></span><br><span class="line">down_stack = tf.keras.Model(inputs=base_model.<span class="built_in">input</span>, outputs=base_model_outputs)</span><br><span class="line"></span><br><span class="line">down_stack.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">up_stack = [</span><br><span class="line">    pix2pix.upsample(<span class="number">512</span>, <span class="number">3</span>),  <span class="comment"># 4x4 -&gt; 8x8</span></span><br><span class="line">    pix2pix.upsample(<span class="number">256</span>, <span class="number">3</span>),  <span class="comment"># 8x8 -&gt; 16x16</span></span><br><span class="line">    pix2pix.upsample(<span class="number">128</span>, <span class="number">3</span>),  <span class="comment"># 16x16 -&gt; 32x32</span></span><br><span class="line">    pix2pix.upsample(<span class="number">64</span>, <span class="number">3</span>),   <span class="comment"># 32x32 -&gt; 64x64</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unet_model</span>(<span class="params">output_channels</span>):</span></span><br><span class="line">  inputs = tf.keras.layers.Input(shape=[<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Downsampling through the model</span></span><br><span class="line">  skips = down_stack(inputs)</span><br><span class="line">  x = skips[-<span class="number">1</span>]</span><br><span class="line">  skips = <span class="built_in">reversed</span>(skips[:-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Upsampling and establishing the skip connections</span></span><br><span class="line">  <span class="keyword">for</span> up, skip <span class="keyword">in</span> <span class="built_in">zip</span>(up_stack, skips):</span><br><span class="line">    x = up(x)</span><br><span class="line">    concat = tf.keras.layers.Concatenate()</span><br><span class="line">    x = concat([x, skip])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is the last layer of the model</span></span><br><span class="line">  last = tf.keras.layers.Conv2DTranspose(</span><br><span class="line">      output_channels, <span class="number">3</span>, strides=<span class="number">2</span>,</span><br><span class="line">      padding=<span class="string">&#x27;same&#x27;</span>)  <span class="comment">#64x64 -&gt; 128x128</span></span><br><span class="line"></span><br><span class="line">  x = last(x)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.keras.Model(inputs=inputs, outputs=x)</span><br></pre></td></tr></table></figure>
<p><img src="https://tensorflow.google.cn/tutorials/images/segmentation_files/output_sw82qF1Gcovr_0.png" alt="output_sw82qF1Gcovr_0"></p>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/04/13/Tensorflow学习笔记-2/" target="_blank" title="Tensorflow学习笔记-2">http://xiangweixi.cn/2021/04/13/Tensorflow学习笔记-2/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/04/15/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Tensorflow学习笔记-3
        
      </div>
    </a>
  
  
    <a href="/2021/04/12/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Tensorflow学习笔记(1)</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Image"><span class="nav-number">1.</span> <span class="nav-text">Image</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN"><span class="nav-number">1.1.</span> <span class="nav-text">CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Configure-the-dataset-for-performance"><span class="nav-number">1.2.</span> <span class="nav-text">Configure the dataset for performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-augmentation"><span class="nav-number">1.3.</span> <span class="nav-text">Data augmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Option-1-Make-the-preprocessing-layers-part-of-your-model"><span class="nav-number">1.3.1.</span> <span class="nav-text">Option 1: Make the preprocessing layers part of your model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Option-2-Apply-the-preprocessing-layers-to-your-dataset"><span class="nav-number">1.3.2.</span> <span class="nav-text">Option 2: Apply the preprocessing layers to your dataset</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">1.4.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transfer-learning"><span class="nav-number">1.5.</span> <span class="nav-text">Transfer learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#feature-extraction"><span class="nav-number">1.6.</span> <span class="nav-text">feature extraction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fine-tuning"><span class="nav-number">1.7.</span> <span class="nav-text">fine tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Custom-data-augmentation"><span class="nav-number">1.8.</span> <span class="nav-text">Custom data augmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Apply-augmentation-to-a-dataset"><span class="nav-number">1.9.</span> <span class="nav-text">Apply augmentation to a dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Option-1-Using-tf-data-experimental-Counter"><span class="nav-number">1.9.1.</span> <span class="nav-text">Option 1: Using tf.data.experimental.Counter()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Option-2-Using-tf-random-Generator"><span class="nav-number">1.9.2.</span> <span class="nav-text">Option 2: Using tf.random.Generator</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#image-segmentation"><span class="nav-number">1.10.</span> <span class="nav-text">image segmentation</span></a></li></ol></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright © 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>