<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>百面机器学习-非监督学习 | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="unsupervised learning" />
  
  
  
  
  <meta name="description" content="在现实世界的机器学习任务中，我们有时不会像监督学习那样通过输入预测某个结果，而是想要找到输入中的某种共性特征或者结构，亦或是数据之间的某种关联。无监督学习重要包含两大类算法：数据聚类和特征变量关联，其中，聚类算法是通过多次迭代来找到数据的最优分割，而特征变量关联则是通过利用各种相关性分析方法来找到变量之间的关系。 K均值聚类基本思想是，通过迭代的方式得到K个簇使得聚类结果对应的代价函数最小，一般情">
<meta property="og:type" content="article">
<meta property="og:title" content="百面机器学习-非监督学习">
<meta property="og:url" content="http://xiangweixi.cn/2021/04/25/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="在现实世界的机器学习任务中，我们有时不会像监督学习那样通过输入预测某个结果，而是想要找到输入中的某种共性特征或者结构，亦或是数据之间的某种关联。无监督学习重要包含两大类算法：数据聚类和特征变量关联，其中，聚类算法是通过多次迭代来找到数据的最优分割，而特征变量关联则是通过利用各种相关性分析方法来找到变量之间的关系。 K均值聚类基本思想是，通过迭代的方式得到K个簇使得聚类结果对应的代价函数最小，一般情">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-04-25T05:34:23.000Z">
<meta property="article:modified_time" content="2021-04-27T03:27:34.525Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="unsupervised learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form" method="GET" action="https://www.baidu.com/s?">
    <input name="wd" type="text" class="search-form-input" placeholder="index.search" />
    <button type="submit" class="search-form-submit"></button>
</form>
<script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:xiangweixi.cn ' + keyword;
        return false;
    });
})(jQuery);
</script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-百面机器学习-非监督学习" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      百面机器学习-非监督学习
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/04/25/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" class="article-date">
	  <time datetime="2021-04-25T05:34:23.000Z" itemprop="datePublished">2021-04-25</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>在现实世界的机器学习任务中，我们有时不会像监督学习那样通过输入预测某个结果，而是想要找到输入中的某种共性特征或者结构，亦或是数据之间的某种关联。无监督学习重要包含两大类算法：数据聚类和特征变量关联，其中，聚类算法是通过多次迭代来找到数据的最优分割，而特征变量关联则是通过利用各种相关性分析方法来找到变量之间的关系。</p>
<h2 id="K均值聚类"><a href="#K均值聚类" class="headerlink" title="K均值聚类"></a>K均值聚类</h2><p>基本思想是，通过迭代的方式得到K个簇使得聚类结果对应的代价函数最小，一般情况下，代价函数是各个样本距离所属簇中心的欧式距离和。<br>$$<br>J(c,\mu)=\sum_{i=1}^M||x_i-\mu_{c_i}||^2<br>$$</p>
<blockquote>
<ul>
<li><p>简述K均值算法的具体步骤(较简单)</p>
<p>(1)数据预处理，如归一化、离群点处理等；</p>
<p>(2)随机选取K个初始中心，记作μ<sub>1</sub><sup>(0)</sup>,μ<sub>2</sub><sup>(0)</sup>,…,μ<sub>k</sub><sup>(0)</sup>;</p>
<p>(3)定义代价函数<br>$$<br>J(c,\mu)=\mathop{min}\limits_{\mu}\mathop{min}\limits_{c}\sum_{i=1}^M||x_i-\mu_{c_i}||^2<br>$$<br>(4)令t=0,1,2,…为迭代步数，重复下面过程直到J收敛：</p>
<p>对于每个样本x<sub>i</sub>，将其分配到距离最近的簇<br>$$<br>c_i^{(t)}\leftarrow \mathop{argmin}\limits_{k}||x_i-\mu_k^{(t)}||^2<br>$$<br>对于每个簇k，重新计算该类簇的中心<br>$$<br>\mu_{k}^{(t+1)}\leftarrow \mathop{argmin}\limits_{\mu}\sum_{i:c_i^{t}=k}||x_i-\mu||^2<br>$$<br>迭代中J没有达到最小值，那么就固定簇中心，调整每个样例所属类别；再固定每个样例所属类别，调整簇中心，这样J能够达到单调递减。</p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>K均值算法的优缺点是什么？如何对其进行调优？(一般)</p>
<p>K均值算法的缺点：受初值和离群点的影响较严重；结果通常不是全局最优而是局部最优；无法很好地解决数据簇分布差别比较大的情况；不太适用于离散分类等。</p>
<p>优点：当在大数据集上使用K均值聚类时，它是可伸缩且高效的，计算复杂度接近于线性(O(NKt)，N是数据对象的数目，K是聚类的簇数，t是迭代的轮数)，尽管算法通常是局部最优解，但已经足够满足聚类需求。</p>
<p>k均值算法的调优我们可以从以下几个角度出发：</p>
<p>(1)数据归一化和离群点处理：K均值聚类的本质是通过欧式距离对数据集进行划分，未做数据归一化或没有统一单位的数据都会导致聚类的结果产生严重的偏差。同时，离群点或者少量噪声都会对均值产生较大影响。</p>
<p>(2)合理选择K值：K值作为超参数，是算法最重要的问题之一，一般采用手肘法(前几篇关于Kmeans的blog都有描述)。手肘法作为经验方法，缺点是不够自动化，于是我们可以使用Gap Statistic方法，当聚类数量为K时，损失函数定义为D<sub>k</sub>，于是Gap Statistic定义为：<br>$$<br>Gap(K)=E(logD_k)-logD_k<br>$$<br>当Gap(K)取得最大值的时候，我们获得此时的K值，其中E(logD<sub>k</sub>)作为logD<sub>k</sub>的期望，通过蒙特卡洛模拟产生，我们对样本所在区域按照均匀分布随机产生和原样本数量一样多的随机样本，并对这个样本进行K均值，得到一个D<sub>k</sub>；重复多次求均值就能得到logD<sub>k</sub>近似期望，当Gap最大时，实际上代表的就是随机样本损失和实际样本损失之差最小。</p>
<p>(3)采用核函数：传统的欧式距离度量方式，令K均值算法的本质假设各个数据簇的数据具有一定的先验概率，并呈现球形或者是高维球形分布。面对非凸的数据分布形状，需要我们引入核函数来优化，通过将点非线性映射到高维空间中，并在新的特征空间进行聚类，这会使得点能够被线性可分的概率增加。</p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>针对K均值算法的缺点，有哪些改进的模型？(一般)</p>
<blockquote>
<ul>
<li>K-means++算法：对K值的选择是K均值算法中关键一步，假设我们已经有了n个聚类中心，那么我们选择离这n个聚类中心越远的点有越高的概率被选为第n+1个聚类中心。在选取第1个聚类中心时，我们使用随机选择的方法。</li>
<li>ISODATA算法：用于K值大小不确定时，全称为迭代自组织数据分析法。它的思想很直观：当某类别数量较少，或是两类的聚类中心相距较近，就进行合并，当某类别数量较多、分散程度较大时，就进行分裂操作。ISODATA算法的缺点时需要给定的超参数较多，除了参考聚类数量范围K以外，还需要给定类别最小样本数量、类内最大方差、两聚类中心之间允许的最小距离。</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>证明K均值算法的收敛性(较难)</p>
<p>K均值算法实际上是一种EM算法，EM算法解决的是含有隐变量的模型参数估计问题，m个样本的参数𝛉，最大化对数似然函数可以写成如下函数：<br>$$<br>\theta=\mathop{argmax}\limits_{\theta}\sum_{i=1}^mlogP(x^{(i)}|\theta)<br>$$<br>含有隐变量z时，参数的最大似然估计为<br>$$<br>\theta=\mathop{argmax}\limits_{\theta}\sum_{i=1}^mlog\sum_{x^{(i)}}P(x^{(i)},z^{(i)}|\theta)<br>$$<br>假设z<sup>(i)</sup>的分布为Q(z<sup>(i)</sup>)，使用Jenson不等式，可以得到<br>$$<br>\sum_{i=1}^mlog\sum_{x^{(i)}}P(x^{(i)},z^{(i)}|\theta)=\sum_{i=1}^mlog\sum_{x}Q(z^{(i)})\frac{P(x^{(i)},z^{(i)}|\theta)}{Q(z^{(i)})}≥\sum_{i=1}^m\sum_{x^{(i)}}Q(z^{(i)})log\frac{P(x^{(i)},z^{(i)}|\theta)}{Q(z^{(i)})}<br>$$<br>想要等式成立，需要满足<br>$$<br>\frac{P(x^{(i)},z^{(i)}|\theta)}{Q(z^{(i)})}=c<br>$$<br>此时我们相当于为我们的优化目标找到了一个下界，根据函数单调有界必收敛，我们能够证明EM算法的收敛性。只是EM算法保证收敛的局部最优值。</p>
<p>K均值算法实际上就是EM算法求解一个特殊的含隐变量的最大似然问题，最后推导能够得到<br>$$<br>\sum_{i=1}^m\sum_{x^{(i)}}Q(z^{(i)})log\frac{P(x^{(i)},z^{(i)}|\theta)}{Q(z^{(i)})}=const-||x^{(i)}-\mu_{z^{(i)}}||^2<br>$$<br>这一步等同于找到最优中心点使得损失函数最小。</p>
</li>
</ul>
</blockquote>
<h2 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h2><p>高斯混合模型假设数据的分布是由多个高斯分布叠加的结果。理论上，高斯混合模型能够拟合出任意分布。</p>
<blockquote>
<ul>
<li><p>高斯混合模型的核心思想是什么？它是如何迭代计算的？(较简单)</p>
<p>核心思想是，可以将数据看作是多个高斯分布中生成的，这样的假设让每个分模型都称为标准高斯模型，它们的均值与方差是待估计参数，每个分模型具有一个参数𝝰<sub>i</sub>，可以看作是权重或是生成数据的概率，可以得到它的公式为<br>$$<br>p(x)=\sum_{i=1}^K\alpha_iN(x|\mu_i,\sigma_i)<br>$$<br>高斯混合模型也需要我们给定K的数量，此时我们将问题转变成了最佳的均值、方差、权重的寻找，GMM模型通常也是使用EM算法进行优化：固定一个变量是的整体变为凸优化函数，求导得到最优值，再利用最优参数更新被固定的变量，然后进入下一轮迭代。具体步骤为：首先随机初始化参数，然后重复一下两步直到收敛：1.E步，根据当前参数，计算每个点由某个分模型生成的概率；2.M步，使用E步估计出的概率对各个模型的均值、方差和权重进行优化。</p>
<p>更详细的来讲，就是我们不知道这三个参数，每次循环时就先使得当前分布不变，然后获得生成概率，再通过这个生成概率和数据点去优化三个参数，循环往复直到收敛，我们就能得到比较合理的一组高斯分布。</p>
<p>GMM与K-means的相同点是它们都通过EM算法进行优化，并且都需要给定聚类数量K，往往都只能收敛于局部最优。而它相对于Kmeans的优点在于它能够给出数据点分属哪一类的概率，还可以用于生成数据点。</p>
</li>
</ul>
</blockquote>
<h2 id="自组织映射神经网络-self-organizing-map-SOM"><a href="#自组织映射神经网络-self-organizing-map-SOM" class="headerlink" title="自组织映射神经网络(self-organizing map,SOM)"></a>自组织映射神经网络(self-organizing map,SOM)</h2><p>它是无监督学习方法中的一种，常用于聚类、高维度可视化、数据压缩、特征提取等多种用途，在深度学习大行其道的今天，SOM由于使用了大量人脑神经元处理信号的机制，仍然有着不容忽视的作用。</p>
<blockquote>
<ul>
<li><p>自组织映射神经网络是如何工作的？它与K均值算法有何区别？(一般)</p>
<p>生物学研究表明，在人脑的感知通道上，神经元组织是有序排列的；同时，大脑皮层会对外界特定输入在特定区域产生兴奋，同时相近的输入会导致相近的区域产生兴奋，即类似的外界输入产生兴奋的大脑皮层区域也是连续映像的。也就是说当你观看一段内容连续的视频时，或是收听频率相近的声音时，位置相邻的神经元兴奋相近。这样的响应特点并非先天安排好的，而是通过后天的学习自组织而成的。</p>
<p>而生物神经系统中，实际上还存在一种侧抑制现象，即一个神经细胞兴奋以后，会对周围其他神经细胞产生抑制作用，这会导致一种竞争，表现出来就是获胜的细胞兴奋，竞争失败的细胞受到抑制。自组织神经网络就是基于上述原理的人工神经网络模拟。</p>
<p>SOM本质上是一个两层的神经网络，包含输入层以及输出层(竞争层)。输入层模拟感知器获取外界输入，输出层模拟做出相应的大脑皮层。输出层中神经元个数通常是聚类的数量，SOM的训练方式时“竞争学习”，即每个输入样本在输出层中寻找与它最为匹配的神经元，该神经元被称为激活节点，也叫winning neuron；然后使用随机梯度下降法更新激活节点的参数，并同时更新和激活节点较近的点。这种竞争可以通过横向抑制实现。SOM模型输出层可以是多维的，它的神经元之间存在拓扑关系。</p>
<p>假设输入空间是D维，输入模式为x={x<sub>i</sub>,i=1,…,D}，输入单元i和神经元j之间在计算层的连接权重为w={w<sub>i,j</sub>,j=1,…,N,i=1,…,D}，其中N是神经元综述，SOM自组织学习过程可以归纳为以下几个子过程：</p>
<p>(1)初始化：所有连接权重都用小的随机值进行初始化；</p>
<p>(2)竞争：神经元计算每一个输入模式各自的判别函数值，并宣布具有最小判别函数值的特定神经元为胜利者，其中每个神经元j的判别函数为<br>$$<br>d_j(x)=\sum_{i=1}^D(x_i-w_{i,j})^2<br>$$<br>(3)合作：获胜神经元I(x)决定了兴奋神经元拓扑领域的空间位置，确定激活节点后，我们更新它以及临近节点<br>$$<br>T_{j,I(x)}(t)=e^{\frac{-S^2_{j,I(x)}}{2\sigma(t)^2}}<br>$$<br>S<sub>i,j</sub>代表竞争层两神经元之间的距离，𝛔(t)=𝛔<sub>0</sub>exp(-t/𝝉<sub>𝛔</sub>)随时间衰减；</p>
<p>(4)适当调整相关兴奋神经元连接权重，使得获胜的神经元对相似输入模式的后续应用响应增强：<br>$$<br>\Delta W_{ji}=\eta(t)<em>T_{j,I(x)}(t)</em>(x_i-w_{ji})<br>$$<br>其中学习率依赖于时间，满足：<br>$$<br>\eta(t)=\eta_0exp(-\frac{t}{\tau_n})<br>$$<br>(5)迭代，返回步骤(2)，直到特征映射趋于稳定。</p>
<p>迭代结束后，每个样本对应的激活神经元就是它的类别。SOM具有保序映射的特点，可以将任意维输入在输出层映射为一维或二维图形，并保持拓扑结构不变。由其学习过程可以看出，每个学习权重更新的效果等同于将获胜的神经元及其邻近的连接权重向量向着输入逼近，同时保证网络的拓扑有序。</p>
<p>SOM中的竞争学习，使得近邻神经元受激励，远邻神经元受抑制，更远邻受弱激励。</p>
<p>SOM与K-means的区别：</p>
<p>(1)SOM无需指定K值，竞争层中的神经元个数不代表最终聚类数量，因为有些神经元可以没有任何输入数据与之对应；</p>
<p>(2)K均值是更新类的参数，SOM更新网络节点的参数，因此K-means受噪声影响较大，而SOM可能会在准确性上有欠缺；</p>
<p>(3)相比较而言，SOM具有更好的可视化效果，而且有一个拓扑的关系图。</p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>怎样设计SOM并设定网络训练参数？(一般)<ul>
<li>设定输出层神经元数量：SOM输出层神经元数量与聚类数量相关，如果不知道类别数，尽量将输出层神经元数量设计的大一些，如果出现样本数量过少的类别或是“死节点”，再适当减少输出层神经元。</li>
<li>设计输出层节点排列：输出层节点如何排列，取决于实际应用的需求，一般来说是需要直观反映问题的物理意义。比如分类问题，使用一维排列，结构简单明了；而颜色空间或者旅行途径使用二维平面比较直观。</li>
<li>初始化权值：随机初始化时应该使权值的初始位置与输入样本的大概分布区域充分重合，避免出现大量的初始“死节点”，一般我们使用随机抽取样本进行初始化来解决这个问题。</li>
<li>设计拓扑领域：设计原则是能够使领域不断缩小，这样可以让输出层的相邻神经元既有相似部分也有不同，从而保证了激活节点的近邻激励。领域形状可以是正方形、六边形或菱形，优势领域的大小用领域半径表示，通常凭借经验选择。</li>
<li>设计学习率：学习率是一个单调递减的数据，它的更新可以和权值更新一同考虑。训练开始时选择较大学习率，之后较快速度下降，这样能够很快地捕捉到输入数据的大致结构，然后再缓慢地降到0，这样能够对权值进行微调。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="聚类算法的评估"><a href="#聚类算法的评估" class="headerlink" title="聚类算法的评估"></a>聚类算法的评估</h2><blockquote>
<ul>
<li><p>假设没有任何外部标签数据，如何评估两个聚类算法的？(一般)</p>
<p>数据的聚类依赖与实际需求，没有外部标签时，我们通过数据的特征度量以及评估数据相似性的方法，来比较不同算法的性能优劣。我们首先需要了解常见的数据簇的特点。</p>
<ul>
<li><p>以中心定义的数据簇：这类数据集合倾向于球形分布，通常中心被定义为质心。</p>
</li>
<li><p>以密度定义的数据簇：这类数据簇与簇之间有着明显不同的密度，或稠密或稀疏。当聚类数据不规则或者相互缠绕，并且有噪声、离群点时，常常使用基于密度的簇定义。</p>
</li>
<li><p>以连通定义的数据簇：数据集合中的数据点与数据点之间有连接关系，整个数据簇表现为图结构。该定义对不规则形状或者缠绕的数据簇有效。</p>
</li>
<li><p>以概念定义的数据簇：满足某种特定关系的数据点聚为一类。</p>
</li>
</ul>
<p>聚类算法性能的评估是一个比较主观的行为，但也是必要的，我们通过评估能够得到在数据集上进行聚类的可行性，聚类方法产生结果的质量，这一过程可以分为三个子任务：</p>
<p>(1)估计聚类趋势：这一步骤是为了检测数据分布中是否存在非随机的簇结构，如果我们的数据是随机分布的，那么聚类将没有任何意义。我们可以通过观察不同K值情况下，聚类的损失函数变化，正常来说随着K值增加，损失函数变化应该较为明显，如果损失函数一直基本没变化，那么说明数据是随机分布的。</p>
<p>我们也可以使用霍普金斯随机量(Hopkins Statistic)来判断空间上的随机性。首先从所有样本中找n个点，记为p<sub>1</sub>,p<sub>2</sub>,…,p<sub>n</sub>，对其中的每一个点p<sub>i</sub>，都在样本空间找到一个离它一个离它最近的点并计算它们之间的距离x<sub>i</sub>，然后从样本可能的取值范围中，随机生成n个点，记为q<sub>i</sub>，计算离它最近的样本点的距离，得到y<sub>i</sub>，霍普金斯统计量可以表示为：<br>$$<br>H=\frac{\sum_{i=1}^ny_i}{\sum_{i=1}^ny_i+\sum_{i=1}^nx_i}<br>$$<br>假如样本接近随机分布，那么H的值接近于0.5，假如聚类趋势明显，那么H的值接近于1。</p>
<p>(2)判定数据簇数：确定数据簇数的方法之前K均值也提到过，主要就是手肘法和Gap statistic方法，但值得一提的是，有些能够自己确定聚类数量的算法得出的聚类数量，与我们用上述方法确定的最优聚类数量可能不同。</p>
<p>(3)测定聚类质量：</p>
<p>给定预设的簇数，不同聚类算法输出不同聚类结果，我们怎么判定哪个聚类结果质量更高呢？无监督情况下，我们可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。下面给出一些常见的度量指标：</p>
<ul>
<li><p>轮廓系数：给定一个点p，该点的轮廓系数定义为<br>$$<br>s(p)=\frac{b(p)-a(p)}{max(a(p),b(p))}<br>$$<br>a(p)是点p与同一簇中的其他点p’之间的平均距离；b(p)是点p与另一个不同簇中的点之间的最小平均距离(如果有n个其他簇，则只计算与点p最接近的一簇中的点与该点平均距离)，a(p)反映的是p所属簇中数据的紧凑程度，b(p)反映的是该簇与其他临近簇的分离程度，b(p)越大，a(p)越小，聚类效果越好。通常将s(p)求平均值来度量聚类结果的质量。</p>
</li>
<li><p>均方根标准偏差(Root-mean-square standard deviation,RMSSTD)：用来衡量聚类结果的同质性，即紧凑程度，定义为：<br>$$<br>RMSSTD=[\frac{\sum_i\sum_{x\in C_i}||x-c_i||^2}{P\sum_i(n_i-1)}]^{\frac{1}{2}}<br>$$<br>C<sub>i</sub>代表第i个簇，c<sub>i</sub>是该簇中心，n<sub>i</sub>是第i个簇的样本点，P为样本点对应的向量维数。RMSSTD对数据的维度进行了惩罚，维度越高，则总体平方距离度量值越大。综上，RMSSTD可以看作是经过归一化的标准差。</p>
</li>
<li><p>R方(R-square)：可以用来衡量聚类的差异度，定义为<br>$$<br>RS=\frac{\sum_{x\in D}||x-c||^2-\sum_i\sum_{x\in C_i}||x-c_i||^2}{\sum_{x\in D}||x-c||^2}<br>$$<br>D代表整个数据集，c代表数据集D的中心。RS代表了聚类之后的结果相较于聚类之前，平方误差和指标的改进程度。</p>
<ul>
<li>改进的Hubert统计：通过数据对的不一致性来评估聚类差异，定义为<br>$$<br>\Gamma=\frac{2}{n(n-1)}\sum_{x\in D}\sum_{y\in D}d(x,y)d_{x\in C_i,y\in C_j}(c_i,c_j)<br>$$<br>d(x,y)表示点x到点y之间的距离，这个统计量值越大说明聚类结果与样本原始距离越吻合，也就是聚类质量越高。</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/04/25/百面机器学习-非监督学习/" target="_blank" title="百面机器学习-非监督学习">http://xiangweixi.cn/2021/04/25/百面机器学习-非监督学习/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2021/04/22/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%99%8D%E7%BB%B4/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">百面机器学习-降维</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="nav-number">1.</span> <span class="nav-text">K均值聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">高斯混合模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%BB%84%E7%BB%87%E6%98%A0%E5%B0%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-self-organizing-map-SOM"><span class="nav-number">3.</span> <span class="nav-text">自组织映射神经网络(self-organizing map,SOM)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="nav-number">4.</span> <span class="nav-text">聚类算法的评估</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright © 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>