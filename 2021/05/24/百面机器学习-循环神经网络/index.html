<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>百面机器学习-循环神经网络 | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="machine learning" />
  
  
  
  
  <meta name="description" content="对于循环神经网络(Recurrent Neural Network,RNN)来说，处理序列化数据是它擅长的东西。传统的前馈神经网络一般输入都是具有尺寸要求的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN通过神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据">
<meta property="og:type" content="article">
<meta property="og:title" content="百面机器学习-循环神经网络">
<meta property="og:url" content="http://xiangweixi.cn/2021/05/24/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="对于循环神经网络(Recurrent Neural Network,RNN)来说，处理序列化数据是它擅长的东西。传统的前馈神经网络一般输入都是具有尺寸要求的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN通过神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tse4-mm.cn.bing.net/th/id/OIP.I9qvwkinYTFGwD5SNfnL6wHaF7?pid=ImgDet&rs=1">
<meta property="article:published_time" content="2021-05-24T07:20:10.000Z">
<meta property="article:modified_time" content="2021-05-28T03:09:19.299Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tse4-mm.cn.bing.net/th/id/OIP.I9qvwkinYTFGwD5SNfnL6wHaF7?pid=ImgDet&rs=1">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form" method="GET" action="https://www.baidu.com/s?">
    <input name="wd" type="text" class="search-form-input" placeholder="index.search" />
    <button type="submit" class="search-form-submit"></button>
</form>
<script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:xiangweixi.cn ' + keyword;
        return false;
    });
})(jQuery);
</script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-百面机器学习-循环神经网络" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      百面机器学习-循环神经网络
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/05/24/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
	  <time datetime="2021-05-24T07:20:10.000Z" itemprop="datePublished">2021-05-24</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>对于循环神经网络(Recurrent Neural Network,RNN)来说，处理序列化数据是它擅长的东西。传统的前馈神经网络一般输入都是具有尺寸要求的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN通过神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。</p>
<h2 id="循环神经网络和卷积神经网络"><a href="#循环神经网络和卷积神经网络" class="headerlink" title="循环神经网络和卷积神经网络"></a>循环神经网络和卷积神经网络</h2><p>用传统方法进行文本分类任务，通常将一篇文章对应的TF-IDF向量作为输入，其中TF-IDF向量的维度是词汇量的大小，使用前馈神经网络和循环神经网络对文本这样的有序列信息的数据进行建模有什么不同？</p>
<blockquote>
<ul>
<li><p>处理文本数据时，循环神经网络与前馈神经网络相比有什么特点？(容易)</p>
<p>传统的文本处理任务将TF-IDF向量作为特征输入，这样的输入明显是将文本这样的有序列信息数据作为无序列信息处理，丢失了有序数据，而卷积神经网络可以学习到原文本中的一些局部特征，但是两个单词之间的长距离依赖关系还是很难被学习到。循环神经网络却能够很好处理文本数据变长并且有序的输入序列。它摸你了人阅读一篇文章的顺序，从前到后阅读文章中的每一个单词，将前面阅读到的有用信息编码到状态变量中去，从而拥有了一定的记忆能力。</p>
<p>由图可见，一个长度为T的序列用循环神经网络建模，展开之后可以看作是一个T层的前馈神经网络。其中第t层的隐含状态h<sub>t</sub>编码了序列中前t个输入的信息，可以通过当前的输入x<sub>t</sub>和上一层神经网络的状态h<sub>t-1</sub>计算得到；最后一层的状态h<sub>T</sub>编码了整个序列的信息，可以作为整篇文档的压缩表示，以此为基础的结构可以应用于多种具体任务。例如，在h<sub>T</sub>后面直接接一个Softmax层，输出文本所属类别的预测概率y，就可以实现文本分类，它们的计算公式分别为<br>$$<br>net_t=Ux_t+Wh_{t-1}<br>$$</p>
<p>$$<br>h_t=f(net_t)<br>$$</p>
<p>$$<br>y=g(Vh_T)<br>$$</p>
<p>其中f和g为激活函数，U为输入层到隐含层的权重矩阵，W为隐含层从上一时刻到下一时刻状态转移的权重矩阵。文本分类任务中，f一般选取Tanh函数或者ReLU函数，g可以采用Softmax函数。</p>
<p>通过最小化损失误差，我们即可训练RNN，它具备对序列顺序信息的刻画能力，因此往往可以得到更准确的结果。</p>
</li>
</ul>
</blockquote>
<h2 id="循环神经网络的梯度消失问题"><a href="#循环神经网络的梯度消失问题" class="headerlink" title="循环神经网络的梯度消失问题"></a>循环神经网络的梯度消失问题</h2><p>1991年，深度学习的发展达到冰点，BP算法被指出存在梯度消失问题(Gradient Vanishing)问题，即在梯度的反向传播过程中，后层的梯度以连乘的形式叠加到前层。由于当时神经网络中的激活函数一般都使用Sigmoid函数，而它具有饱和特性，在输入达到一定值的情况下，输出就不会发生明显变化了。而后层梯度本来就比较小，误差梯度反传到前层时一般会衰减为0，因此无法对前层的参数进行有效学习。</p>
<blockquote>
<ul>
<li><p>循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？(较容易)</p>
<p>循环神经网络模型的求解可以采用BPTT(Back Propagation Through Time，基于时间的反向传播)算法实现，BPTT实际上是反向传播算法的简单变种。如果将循环神经网络按照时间展开成为T层的前馈神经网络来理解，那么就和普通的反向传播算法没什么区别。循环神经网络设计初衷之一就是可以捕获长距离输入之间的依赖。从结构上来看，它也理应做到这点，但实践中发现，使用BPTT算法学习的RNN并不能捕获长距离的依赖关系，这是由于深度神经网络中BP算法的梯度消失，RNN的神经网络梯度为<br>$$<br>\frac{\part net_t}{\part net_1}=\frac{\part net_t}{\part net_{t-1}}.\frac{\part net_{t-1}}{\part net_{t-2}}…\frac{\part net_2}{\part net_{1}}<br>$$<br>其中<br>$$<br>\frac{\part net_t}{\part net_{t-1}}=\frac{\part net_t}{\part h_{t-1}}\frac{\part h_{t-1}}{\part net_{t-1}}=W.diag[f’(net_{t-1})]<br>$$<br>其中n为隐含层h<sub>t-1</sub>的维度(即隐含单元的个数)，梯度连乘形式对应的nxn维矩阵，又被称为Jacob矩阵。</p>
<p>由于预测的误差是沿着神经网络的每一层反向传播的，因此当Jacob的最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸；反之，若Jacob矩阵的最大特征值小于1，梯度的大小会呈指数缩小，产生梯度消失。对于普通的前馈网络来说，梯度消失意味着无法通过加深网络层次来改善神经网络的预测效果，因为只有靠近输出的浅层网络能够起到真正的学习作用，这使得循环神经网络模型很难学习到输入序列中的长距离依赖关系。</p>
<p>梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值时，对梯度进行等比收缩。而梯度消失问题相对比较棘手，需要对模型本身进行改进。深度残差网络是对前馈神经网络的改进，通过残差学习的方式缓解了梯度消失的现象，从而使得我们能够学习到更深层的网络表示；而对于循环神经网络来说，长短时记忆模型(LSTM)和变种门控循环单元(Gated recurrent unit,GRU)等模型通过加入门控机制，很大程度上弥补梯度消失所带来的损失。</p>
</li>
</ul>
</blockquote>
<h2 id="循环神经网络中的激活函数"><a href="#循环神经网络中的激活函数" class="headerlink" title="循环神经网络中的激活函数"></a>循环神经网络中的激活函数</h2><p>在卷积神经网络等前馈神经网络中采用ReLU函数作为激活函数，那么在循环神经网络中可以这么做吗？</p>
<blockquote>
<ul>
<li><p>在循环神经网络中是否可以使用ReLU函数作为激活函数？(一般)</p>
<p>答案是可以，但是需要对矩阵的初值做一定的限制，我们回顾一下循环神经网络的前向前向传播公式<br>$$<br>net_t=Ux_t+Wh_{t-1}<br>$$</p>
<p>$$<br>h_t=f(net_t)<br>$$</p>
<p>根据前向传播公式向前传递一层，可以的得到<br>$$<br>net_t=Ux_t+Wh_{t-1}=Ux_t+Wf(Ux_{t-1}+Wh_{t-2})<br>$$<br>使用ReLU公式替代f，并且假设ReLU函数一直处于激活区，即输入大于0，我们会得到<br>$$<br>f(x)=x,net_t=Ux_t+W(Ux_{t-1}+Wh_{t-2})<br>$$<br>继续展开，我们会发现表达式中包含t个W连乘的形式，除非权重矩阵是单位矩阵，否则多层以后元素会趋向无穷或者是趋向0，而卷积神经网络不会出现这种情况的原因是卷积神经网络的权重矩阵是不同的，而且在初始化时权重矩阵是独立同分布的，可以相互抵消。</p>
<p>再回到循环神经网络的梯度计算公式<br>$$<br>\frac{\part net_t}{\part net_{t-1}}=W.diag[f’(net_{t-1})]<br>$$<br>假设使用ReLU激活函数，且一开始所有的神经元都处于激活状态(所有输入大于0)，于是最后的梯度计算公式结果为W<sup>n</sup>，即使采用了ReLU激活汗水，当W不是单位矩阵时，仍然会出现梯度消失或梯度爆炸。</p>
<p>综上，当使用ReLU激活函数时，我们需要将权重矩阵的取值在单位矩阵附近才能取得较好的结果，而实验结果证明，初始化W为单位矩阵并使用ReLU激活函数，可以得到长短期记忆模型相似的结果，并且学习速度比长短期记忆模型速度更快。</p>
</li>
</ul>
</blockquote>
<h2 id="长短期记忆网络"><a href="#长短期记忆网络" class="headerlink" title="长短期记忆网络"></a>长短期记忆网络</h2><p>长短期记忆网络(Long Short Term Memory, LSTM)是循环神经网络最知名和成功的扩展。由于循环神经网络有梯队消失和梯度爆炸的问题，学习能力有限，在实际任务中的效果往往达不到效果。而LSTM可以对有价值信息进行长短期记忆，从而减小循环神经网络学习难度。</p>
<blockquote>
<ul>
<li><p>LSTM是如何实现长短期记忆功能的？(较容易)</p>
<p><img src="https://tse4-mm.cn.bing.net/th/id/OIP.I9qvwkinYTFGwD5SNfnL6wHaF7?pid=ImgDet&rs=1" alt="See the source image"></p>
<p>我们可以结合LSTM结构图以及更新的计算公式探讨这种网络如何实现其功能。</p>
<p>与传统的RNN相比，LSTM依然通过x<sub>t</sub>和h<sub>t-1</sub>来计算h<sub>t</sub>，不过对于内部的结构进行了更精心的设计，加入了输入门i<sub>t</sub>、遗忘门f<sub>t</sub>以及输出门o<sub>t</sub>三个门和一个内部记忆单元c<sub>t</sub>。输入门控制当前计算的新状态以多大程度更新到记忆单元中；遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉；输出门控制当前的输出有多大程度上取决于当前的记忆单元。</p>
<p>经典LSTM中，第t步更新计算公式为<br>$$<br>i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)<br>$$</p>
<p>$$<br>f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)<br>$$</p>
<p>$$<br>o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)<br>$$</p>
<p>$$<br>\tilde{c}_t=Tanh(W_cx_t+U_ch_{t-1})<br>$$</p>
<p>$$<br>c_t=f_t\bigodot c_{t-1}+i\bigodot \tilde{c}_t<br>$$</p>
<p>$$<br>h_t=o_t\bigodot Tanh(c_t)<br>$$</p>
<p>其中i<sub>t</sub>是通过输入x<sub>t</sub>和上一步的隐含层输出h<sub>t-1</sub>进行线性变换，再经过激活函数得到的。 输入门结果是向量，其中每个元素取值范围在[0,1]，用于控制各维度流过阀门的信息量；W<sub>i</sub>,U<sub>i</sub>两个矩阵以及向量b<sub>i</sub>为输入门的参数，是在训练过程中需要学习得到的。遗忘门f和输出门o的计算方式与输出门类似，它们有各自的参数W,U和b。与传统的循环神经网络不同的是，从上一个记忆单元的状态c<sub>t-1</sub>到当前状态c<sub>t</sub>的转移不一定完全取决于激活函数计算得到的状态，还由输入门和遗忘门来共同控制。</p>
<p>在一个训练好的网络中，当输入的序列没有重要信息时，LSTM的遗忘门的值接近于1，输入门的值接近于0，此时过去的记忆会被保存从而实现了长期记忆功能；当输入序列中出现了重要信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近于1；当输入序列出现了重要信息，且该信息意味着之前的信息不再重要时，输入门的值接近于1，而遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。</p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>LSTM各模块分别使用什么样的激活函数，可以使用别的激活函数吗？(一般)</p>
<p>LSTM中，遗忘门、输入门和输出门使用Sigmoid函数作为激活函数；生成候选记忆时，使用双曲正切函数Tanh作为激活函数。值得注意的是，这两个激活函数都是饱和的，也就是说在输入达到一定值的情况下，输出就不会发生明显改变了。如果使用非饱和激活函数，例如ReLU，那么将难以实现门控的效果。Sigmoid函数的输出在0-1之间，符合门控的物理定义。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。在生成候选记忆时，使用Tanh函数，是因为其输出在-1～1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh函数在输入为0附近相比于Sigmoid函数有更大的梯度，通常使模型收敛更快。</p>
<p>激活函数的选择也不是一成不变的。例如在原始的LSTM中，使用的激活函数Sigmoid函数的变种，h(x)=2sigmoid(x)-1，g(x)=4sigmoid(x)-2，这两个函数的范围分别是[-1,1]和[-2,2]。并且在原始的LSTM中，只有输入门和输出门，没有遗忘门，其中输入经过输入门后直接与记忆相加的，所以输入门控g(x)的值是0中心的。后来经过大量的研究和实验，人们发现增加遗忘门对LSTM的性能有很大的提升，并且h(x)使用Tanh比2*Sigmoid(x)-1要好，现在的LSTM都适用Sigmoid和Tanh函数作为激活函数。在门控中使用Sigmoid函数是几乎所有现代神经网络模块的共同选择。在门控单元和注意力机制(Attention)中，使用的也是Sigmoid作为激活函数。</p>
<p>在某些计算资源有限的设备(如可穿戴设备中)，由于Sigmoid函数求指数需要一定计算量，这时会直接采用0/1门(hard gate)让门控输出为0或1的离散值，即输入小于阈值输出0，大于阈值输出1。这样能够在性能不显著下降的情况下减少计算量。经典LSTM在计算各门控，通常使用输入x和隐层输出h参与门控计算，例如对于输入门的更新。其最常见的变种是加入了窥孔机制，让记忆c<sub>t-1</sub>也参与到了门控的计算中，此时输入门的更新方式变为<br>$$<br>i_t=\sigma(W_ix_t+U_ih_{t-1}+V_ic_{t-1}+b_i)<br>$$<br>LSTM经历了20年发展，核心思想就是门控思想。</p>
</li>
</ul>
</blockquote>
<h2 id="Seq2Seq模型"><a href="#Seq2Seq模型" class="headerlink" title="Seq2Seq模型"></a>Seq2Seq模型</h2><p>Seq2Seq，全称Sequence to Sequence模型，目前暂时没有较好的翻译，暂且称之为序列到序列模型。大致就是通过编码和解码，将一个序列信号生成新的序列信号，常用语机器翻译、语音识别、自动对话等任务。Seq2Seq模型相较于普通的深度神经网络的优势是可以处理长度未知的序列。</p>
<blockquote>
<ul>
<li><p>什么是Seq2Seq？它有哪些优点？(较容易)</p>
<p>Seq2Seq模型核心在于，通过深度神经网络将作为输入的序列映射称为一个座位输出的序列，这一过程由编码输入与解码输出两个环节构成。在经典的实现中，编码器和解码器各由一个循环神经网络构成，传统RNN与LSTM、GRU都能够作为编码器和解码器。在Seq2Seq中，两个循环神经网络是共同训练的。</p>
<p>编码与解码的过程就像是复习与考试的过程，循环神经网络通过训练形成所谓知识体系，考试时将高度抽象的知识运用，这就是编码与解码。而普通的深度神经网络不能够记忆长距离、长时间的信息，因此它的编码和解码都是短时效信息。</p>
<p>对应于机器翻译过程，对于输入单词A,B,C，编码器依次读入A,B,C以及结尾符&lt;EOS&gt;。解码器将编码器的最终状态读入，第二步读入第一步的输出W，生成第二个词X；如此循环往复，直至输出结尾符&lt;EOS&gt;。输出的序列W,X,Y,Z就是翻译后目标语言的句子。</p>
<p>不同场景中，编码器和译码器有不同的设计，但对应Seq2Seq的底层结构却如出一辙。</p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>Seq2Seq模型在解码时，有哪些常用的办法?(一般)</p>
<p>Seq2Seq模型最核心的部分是其解码部分，大量改进都是在解码环节衍生的。最基础的解码方法是贪心法，即选定判定标准后，每次都选择当前状态下的最佳结果。它是一种基准方法，计算代价低，但只能获得一个局部最优解。</p>
<p>集束搜索是常见的一种改进，它是一种启发式算法，通过保存beam size个当前较佳选择，然后解码时每一步根据保存的选择进行下一步的扩展和排序，选择前beam size个进行保存，循环迭代后在结束时选择最佳的一个作为解码结果。</p>
<p>在实际应用中，为了平衡解码器性能以及计算复杂性，通常将beam size限制在8～12。</p>
<p>解码时使用堆叠的RNN、增加Dropout机制、与编码器之间建立残差连接等，都是常见的改进措施，另一个改进就是注意力机制以及采用记忆网络。</p>
</li>
</ul>
</blockquote>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力机制(Attention Mechanism)实际上是源于模拟人脑的机制。研究发现人脑在工作上是有注意力存在的，当欣赏一幅艺术作品时，我们深入关注细节时眼睛只聚焦在画幅很小的一部分。说明大脑在处理信号时是有一定权重分配的，注意力机制也就是模仿了大脑的这种核心特性。卷积神经网络特点也是关注了局部特征，它与注意力机制有什么不同？我认为是注意力机制具有权重分配的步骤而卷积神经网络没有。</p>
<blockquote>
<ul>
<li><p>Seq2Seq模型引入注意力机制是为了解决什么问题？为什么选用了双向的循环神经网络?(较难)</p>
<p>实际任务比如机器翻译中，使用Seq2Seq模型，通常会先使用一个循环神经网络作为编码器，将输入序列编码成为一个向量表示，然后再使用一个循环神经网络模型作为解码器，从编码器得到的向量表示里解码得到输出序列。</p>
<p>在Seq2Seq模型中，当前隐状态以及上一个输出词决定了当前输出词，即<br>$$<br>s_i=f(y_{i-1},s_{i-1})<br>$$</p>
<p>$$<br>p(y_i|y_1,y_2…y_{i-1})=g(y_{i-1},s_i)<br>$$</p>
<p>其中f和g时非线性变换，通常是多层神经网络；y<sub>i</sub>是输出序列中的一个词，s<sub>i</sub>是对应的隐状态。</p>
<p>实际使用中会发现，随着输入序列长度的增加，模型的性能显著下降，这是因为编码时向量表示的性能随着输入序列长度的增加而下降。解码时第一步通常会考虑源语言的第一个单词，那么就会考虑序列长度步数前的信息，建模时改善的技巧是将源语言你需输入，或者重复输入。使用LSTM模型也能够达到改善效果。但还是达不到理想效果，因为在解码时，当前词及对应语言词的上下文信息和位置信息在编解码过程中丢失了。</p>
<p>Seq2Seq模型引入Attention mechanism就是为了解决这个问题，依然使用普通RNN进行编码，但在解码时，每个输出词都取决于前一个隐状态以及输入序列每一个对应的隐状态。<br>$$<br>s_i=f(s_{i-1},y_{i-1},c_i)<br>$$</p>
<p>$$<br>p(y_i|y_1,y_2…y_{i-1})=g(y_{i-1},s_i,c_i)<br>$$</p>
<p>其中语境向量c<sub>i</sub>是输入序列全部隐状态h<sub>1</sub>，h<sub>2</sub>…h<sub>T</sub>的一个加权和<br>$$<br>c_i=\sum_{j=1}^T\alpha_{ij}h_j<br>$$<br>其中注意力权重参数并不是固定权重，而是由另个神经网络计算得到的<br>$$<br>\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^Texp(e_{ik})}<br>$$</p>
<p>$$<br>e_{ij}=a(s_{i-1},h_j)<br>$$</p>
<p>神经网络将上一个输出序列隐状态和输入序列隐状态作为输入，计算出一个和x,y值对其的值e，然后归一化得到权重。使用双向循环神经网络，不仅可以得到输入序列头到当前的信息，还可以得到输入序列尾到当前的信息，防止了前后文信息的丢失。</p>
<p>注意力机制是一种思想，可以有多种不同的实现方式，在Seq2Seq以外的场景中也有不少运用。</p>
</li>
</ul>
</blockquote>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/05/24/百面机器学习-循环神经网络/" target="_blank" title="百面机器学习-循环神经网络">http://xiangweixi.cn/2021/05/24/百面机器学习-循环神经网络/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/06/06/%E7%BA%B3%E5%85%B0%E8%AF%8D%E6%91%98/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          纳兰词摘
        
      </div>
    </a>
  
  
    <a href="/2021/05/20/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%89%8D%E5%90%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">百面机器学习-前向神经网络</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">循环神经网络和卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络的梯度消失问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">循环神经网络中的激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">长短期记忆网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2Seq%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">Seq2Seq模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">6.</span> <span class="nav-text">注意力机制</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2022 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright © 2022 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>