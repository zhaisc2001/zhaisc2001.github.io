<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>百面机器学习-优化算法 | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="machine learning" />
  
  
  
  
  <meta name="description" content="优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法&#x3D;模型表征+模型评估+优化算法。优化算法做的是在模型表征空间中国呢找到模型评估指标最好的模型，不同的优化算法对应的模型表征和评估指标不同，我们需要了解优化算法原理。 有监督学习的损失函数机器学习中关键一环是模型评估，损失函数定义了模型的评估指标，没有损失函数我们就无法比较模型，我们要针对具体的问题选取合适的损失函数。   有">
<meta property="og:type" content="article">
<meta property="og:title" content="百面机器学习-优化算法">
<meta property="og:url" content="http://xiangweixi.cn/2021/05/03/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法&#x3D;模型表征+模型评估+优化算法。优化算法做的是在模型表征空间中国呢找到模型评估指标最好的模型，不同的优化算法对应的模型表征和评估指标不同，我们需要了解优化算法原理。 有监督学习的损失函数机器学习中关键一环是模型评估，损失函数定义了模型的评估指标，没有损失函数我们就无法比较模型，我们要针对具体的问题选取合适的损失函数。   有">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-05-03T02:56:31.000Z">
<meta property="article:modified_time" content="2021-05-08T03:07:04.761Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form" method="GET" action="https://www.baidu.com/s?">
    <input name="wd" type="text" class="search-form-input" placeholder="index.search" />
    <button type="submit" class="search-form-submit"></button>
</form>
<script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:xiangweixi.cn ' + keyword;
        return false;
    });
})(jQuery);
</script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-百面机器学习-优化算法" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      百面机器学习-优化算法
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/05/03/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" class="article-date">
	  <time datetime="2021-05-03T02:56:31.000Z" itemprop="datePublished">2021-05-03</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法=模型表征+模型评估+优化算法。优化算法做的是在模型表征空间中国呢找到模型评估指标最好的模型，不同的优化算法对应的模型表征和评估指标不同，我们需要了解优化算法原理。</p>
<h2 id="有监督学习的损失函数"><a href="#有监督学习的损失函数" class="headerlink" title="有监督学习的损失函数"></a>有监督学习的损失函数</h2><p>机器学习中关键一环是模型评估，损失函数定义了模型的评估指标，没有损失函数我们就无法比较模型，我们要针对具体的问题选取合适的损失函数。</p>
<blockquote>
<ul>
<li><p>有监督学习涉及到的损失函数有哪些？请列举并简述它们的特点？(简单)</p>
<p>有监督学习中，损失函数刻画了模型和训练样本的匹配程度。假设训练样本的形式为(x<sub>i</sub>，y<sub>i</sub>)，其中x<sub>i</sub>表示第i个样本的特征，y<sub>i</sub>表示该样本的标签，参数为𝛉的模型可以表示为函数f(.,𝛉):X-&gt;Y，我们定义损失函数为L(.,.):YxY-&gt;R<sub>≥0</sub>，L(f(x<sub>i</sub>,𝛉))越小，表明模型在该样本点匹配得越好。</p>
<p>对二分类问题，Y={1,-1}，我们希望sign f(x<sub>i</sub>,𝛉)=y<sub>i</sub>，最自然的损失函数是0-1损失，即<br>$$<br>L_{0-1}(f,y)=1_{fy≤0}<br>$$<br>其中1<sub>p</sub>是指示函数(Indicator Function)，当且仅当P为真时取值为1，否则取值为0。该损失函数可以直观地刻画分类的错误率，但是由于它非凸、非光滑，对于这个函数的优化比较困难，它的一个代理函数是Hinge损失函数<br>$$<br>L_{hinge}(f,y)=max{0,1-fy}<br>$$<br>Hinge损失函数是0-1损失函数的一个凸上界，它在fy=1处不可导，因此它的优化方法不是SGD，而是次梯度下降法。0-1损失函数的另一个代理函数是Logistic损失函数<br>$$<br>L_{logistic}(f,y)=log_2(1+exp(-fY))<br>$$<br>它也是0-1损失函数的凸上界，而且它处处光滑，因此可以用梯度下降法优化，它对于所有的样本点都有所惩罚，因此logistic损失函数对异常点是敏感的。当预测值在-1到1之间时，我们使用交叉熵损失函数<br>$$<br>L_{crossentropy}(f,y)=-log_2(\frac{1+fy}{2})<br>$$<br>交叉熵损失函数也是0-1损失函数的光滑凸上界。</p>
<p>对于回归问题，我们最常用的损失函数时平方损失函数，它是光滑的，能够使用SGD进行优化，当预测值离实际值偏差较大，平方损失函数惩罚力度加大，因此它也是对于异常点敏感的，我们可以使用绝对损失函数来解决该问题<br>$$<br>L_{absolute}(f,y)=|f-y|<br>$$<br>绝对损失函数问题在于它在f=y处不是光滑的，我们可以使用Huber损失函数，这样既满足了可导性，也满足了对异常点的鲁棒性，Huber损失函数<br>$$<br>L_{Huber}(f,y)=(f-y)^2,when\ |f-y|≤\delta<br>$$</p>
<p>$$<br>L_{Huber}(f,y)=2\delta|f-y|-\delta^2,when\ |f-y|&gt;\delta<br>$$</p>
<p>它在|f-y|较小时为平方损失，在|f-y|较大时为线性损失，处处可导，且对异常点鲁棒。</p>
</li>
</ul>
</blockquote>
<h2 id="机器学习中的优化问题"><a href="#机器学习中的优化问题" class="headerlink" title="机器学习中的优化问题"></a>机器学习中的优化问题</h2><p>大部分机器学习的参数估计问题也可以写成优化问题，机器学习模型不同，损失函数不同，对应的优化问题也不相同。</p>
<blockquote>
<ul>
<li><p>机器学习中的优化问题，哪些时凸优化函数，哪些是非凸优化问题？各举一个例子。(较简单</p>
<p>我们首先对凸函数下一个严格定义，函数是凸函数当且仅当对于定义域中的任意两袋内x,y，和任意实数λ∈[0,1]总有<br>$$<br>L(\lambda x+(1-\lambda x)y)≤\lambda L(x)+(1-\lambda)L(y)<br>$$<br>该不等式的一个直观解释，凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于该函数曲面的下方。（关于凹凸函数，有一个相当变态的事实就是在中文语境下的凹凸函数的定义和由外语翻译而来的凹凸函数的定义是相反的，国内的定义参照同济大学数学系的《高等数学.第七版》（上册）一个函数图像形似“凹”或“凸”则是凹函数或凸函数，而国外的定义参照Cambridge University的Convex Optimization则是函数上方是凸集的是凸函数，函数上方是凹集的是凹函数）</p>
<p>我们将逻辑回归作为一个例子，逻辑回归用于解决二分类问题，假设模型参数为𝛉，则逻辑回归的优化问题为<br>$$<br>\mathop{min}\limits_{\theta}L(\theta)\sum_{i=1}^{n}log(1+exp(-y_i\theta^Tx_i))<br>$$<br>可以通过计算目标函数的二阶Hessian矩阵来验证凸性。</p>
<p>求一阶导，得到<br>$$<br>\triangledown L_i(\theta)=\frac{1}{1+exp(-y_i\theta^Tx_i)}exp(-y_i\theta^Tx_i)(-y_ix_i)=\frac{-y_ix_i}{1+exp(y_i\theta^Tx_i)}<br>$$<br>继续求导，得到Hessian矩阵<br>$$<br>\triangledown^2L_i(\theta)=\frac{y_ix_iexp(y_i\theta^Tx_i)y_ix^T_i}{(1+exp(y_i\theta^Tx_i))}=\frac{exp(y_i\theta^Tx_i)}{(1+exp(y_i\theta^Tx_i))^2}x_ix_i^T<br>$$<br>该矩阵满足半正定的性质，因此函数为凸函数，对于凸优化问题来说，所有的局部极小值都是全局极小值，因此凸优化问题是容易求解的问题。</p>
<p>主成分分析对应的优化问题是非凸优化问题，若数据中心化后的矩阵，主成分分析优化问题<br>$$<br>\mathop{min}\limits_{VV^T=I_k}L(V)=||X-V^TVX||^2_F<br>$$<br>是一个非凸优化问题，一般来说，非凸优化问题是比较难求解的问题。主成分分析是一个特例，因为我们可以使用SVD来直接求得主成分分析的全局极小值。</p>
<p>其他的凸优化问题有支持向量机、线性回归等线性模型，非凸优化问题有矩阵分解(低秩模型)、深度神经网络模型等。</p>
</li>
</ul>
</blockquote>
<h2 id="经典优化算法"><a href="#经典优化算法" class="headerlink" title="经典优化算法"></a>经典优化算法</h2><p>针对不同的优化问题和应用场景，研究者们提出了多种不同的解法，并逐渐发展出了有严格理论支撑的凸优化领域，我们来看一些有助于我们面对新的优化问题得到思路的经典优化算法。</p>
<blockquote>
<ul>
<li><p>无约束优化问题的优化方法有哪些？(较简单)</p>
<p>假设有一道无约束优化问题：<br>$$<br>\mathop{min}\limits_{\theta}L(\theta)<br>$$<br>其中目标函数是光滑的，请问求解该问题的优化算法有哪些？适用场景为？</p>
<p>经典的优化算法可以分为直接法和迭代法两大类。</p>
<p>直接法，顾名思义就是能够直接给出优化问题最优解的方法。虽然听起来很厉害，但在很多时候它都是无法使用的。第一个条件，L是凸函数，此时最优解的条件就是<br>$$<br>\triangledown L(\theta^*)=0<br>$$<br>为了能够求出这个最优解，第二个条件就是，上式有闭式解(解析解)。我们举一个经典的满足这两个条件的例子，岭回归(Ridge Regerssion)，目标函数为<br>$$<br>L(\theta)=||X\theta-y||^2_2+\lambda||\theta||^2_2<br>$$<br>求解得到最优解为<br>$$<br>\theta^*=(X^TX+\lambda I)^{-1}X^Ty<br>$$<br>直接法要满足的这两个条件限制了它的应用范围。因此在很多实际问题中，会采用迭代法，迭代法通过迭代地修正对最优解的估计，假设当前对最优解的估计值为𝛉<sub>t</sub>，希望求解优化问题<br>$$<br>\delta_t=\mathop{argmin}\limits_{\delta}L(\theta_t+\delta)<br>$$<br>得到更好的估计值𝛉<sub>t+1</sub>。迭代法可以分为一阶法和二阶法两类。</p>
<p>一阶法对函数L做一阶泰勒展开，得到近似式<br>$$<br>L(\theta_t+\delta)\approx L(\theta_t)+\triangledown L(\theta_t)^T\delta<br>$$<br>该近似式只有在𝛅较小时才比较准确，因此求解𝛅<sub>t</sub>一般加上L<sub>2</sub>正则项<br>$$<br>\delta_t=\mathop{argmin}\limits_{\delta}(L(\theta_t)+\triangledown L(\theta_t)^T\delta +\frac{1}{2\alpha}||\delta||^2_2)=-\alpha\triangledown L(\theta_t)<br>$$<br>一阶法的迭代公式表示为<br>$$<br>\theta_{t+1}=\theta_{t}-\alpha\triangledown L(\theta_t)<br>$$<br>二阶法对函数L(𝛉<sub>t</sub>+𝛅)做二阶泰勒展开，得到近似式<br>$$<br>L(\theta_t +\delta)\approx L(\theta_t)+\triangledown L(\theta_t)^T\delta+\frac{1}{2}\delta^T\triangledown^2L(\theta_t)\delta<br>$$<br>其中对L求两次导数是L的Hessian矩阵，通过求解近似优化问题<br>$$<br>\delta_t=\mathop{argmin}\limits_{\delta}(L(\theta_t)+\triangledown L(\theta_t)^T\delta+\frac{1}{2}\delta\triangledown^2L(\theta_t)\delta)=-\triangledown^2L(\theta_t)^{-1}\triangledown L(\theta_t)<br>$$<br>可以得到二阶法的迭代公式<br>$$<br>\theta_{t+1}=\theta_t-\triangledown^2L(\theta_t)^{-1}\triangledown L(\theta_t)<br>$$<br>二阶法也称为牛顿法，Hessian矩阵就是目标函数的二阶信息。二阶法的收敛速度一般快于一阶法，高维情况，Hessian矩阵求逆的计算复杂度很大，而且在目标函数非凸时，二阶法可能会收敛到鞍点(Saddle point)。</p>
</li>
</ul>
</blockquote>
<h2 id="梯度验证"><a href="#梯度验证" class="headerlink" title="梯度验证"></a>梯度验证</h2><p>使用梯度下降法，最重要的操作是计算目标函数的梯度，对于较复杂的模型，比如深度神经网络来说，目标函数的梯度公式也会相应地变复杂，因此实际应用中，写出计算梯度的代码后，通常要验证。</p>
<blockquote>
<ul>
<li><p>如何验证求目标函数梯度功能的正确性？(较容易)</p>
<p>根据梯度的定义，我们有<br>$$<br>\nabla L(\theta)=[\frac{\partial L(\theta)}{\partial \theta_1},…,\frac{\partial L(\theta)}{\partial \theta_n}]<br>$$<br>其中对于任意的i=1,2,…,n。梯度的第i个元素的定义为<br>$$<br>\frac{\partial L(\theta)}{\partial \theta_i}=\mathop{lim}\limits_{h\rightarrow0}\frac{L(\theta+he_i)-L(\theta-he_i)}{2h}<br>$$<br>其中e<sub>i</sub>是单位向量，维度和𝛉相同，但只在第i个位置取1，其余位置取0.因此我们将h取一个极接近于0的值，那么偏导数就约等于右式。我们使用泰勒展开来近似该误差，令但变量函数<br>$$<br>\tilde{L}(x)=L(\theta+xe_i)<br>$$<br>根据泰勒展开及拉格朗日余项公式，有<br>$$<br>L(\theta+he_i)=\tilde{L}(h)=\tilde{L}(0)+\tilde{L}’(0)h+\frac{1}{2}\tilde{L}’’(0)h^2-\frac{1}{6}\tilde{L}^{(3)}(p_i)h^3<br>$$<br>其中p<sub>i</sub>∈(0,h)，同理我们也能得到he<sub>i</sub>项前为-时的Lagrange余项泰勒展开，两式相减，等号两边同时除以2h，并且由于<br>$$<br>\tilde{L}’(0)=\frac{\partial L(\theta)}{\partial \theta_i}<br>$$<br>可得<br>$$<br>\frac{L(\theta+he_i)-L(\theta-he_i)}{2h}=\frac{\partial L(\theta)}{\partial\theta_i}+\frac{1}{12}(\tilde{L}^{(3)}(p_i)+\tilde{L}^{(3)}(q_i))h^2<br>$$<br>h充分小时，我们可以近似认为h<sup>2</sup>项前面的系数是常数M，因此近似式的误差为<br>$$<br>|\frac{L(\theta+he_i)-L(\theta-he_i)}{2h}-\frac{\partial L(\theta)}{\partial \theta_i}|\approx Mh^2<br>$$<br>这个近似误差显然是h的高阶无穷小。</p>
<p>实际应用中，我们随机初始化𝛉，取h为较小的数，并对i依次检验<br>$$<br>|\frac{L(\theta+he_i)-L(\theta-he_i)}{2h}-\frac{\partial L(\theta)}{\partial \theta_i}|≤h<br>$$<br>是否成立。如果对于某个下标i不成立，可能的问题是(1)该下标对应的M过大；(2)该梯度分量计算不正确。此时固定𝛉，减小h，并再次计算下表对应的近似误差，若近似误差约减小为减小的h的程度的平方，那么说明是第一个问题，我们应该使用更小的h重新做一次梯度验证；否则对应于第二种可能，我们应该检查求梯度的代码是否有错误。</p>
</li>
</ul>
</blockquote>
<h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><p>经典的优化方法，在每次迭代时需要使用所有的训练数据，这给求解大规模数据的优化问题带来了挑战。</p>
<blockquote>
<ul>
<li><p>当训练数据量特别大时，经典的梯度下降法存在什么问题，需要做如何改进？(容易)</p>
<p>优化问题的目标函数通常可以表示成<br>$$<br>L(\theta)=E_{(x,y)\sim P_{data}}L(f(x,\theta),y)<br>$$<br>模型学习任务为<br>$$<br>\theta^*=argminL(\theta)<br>$$<br>经典的梯度下降法采用所有训练数据的平均损失来近似目标函数，即<br>$$<br>L(\theta)=\frac{1}{M}\sum_{i=1}^ML(f(x_i,\theta),y_i)<br>$$</p>
<p>$$<br>\nabla L(\theta)=\frac{1}{M}\sum_{i=1}^{M}\nabla L(f(x_i,\theta),y_i)<br>$$</p>
<p>M是训练样本的个数，模型参数的更新公式为<br>$$<br>\theta_{t+1}=\theta_t-\alpha \nabla L(\theta_t)<br>$$<br>因此，经典的梯度下降法在每次对模型参数进行更新时需要遍历所有的数据，为了解决该问题，提出了随机梯度下降法(Stochastic Gradient Descent, SGD)用单个训练样本的损失来近似平均损失，即<br>$$<br>L(\theta;x_i,y_i)=L(f(x_i,\theta),y_i)<br>$$</p>
<p>$$<br>\nabla L(\theta;x_i,y_i)=\nabla L(f(x_i,\theta),y_i)<br>$$</p>
<p>因此，随机梯度下降法用单个训练数据也可以对模型参数进行一次更新，大大加快了收敛速率，该方法也非常适用于在线更新场景。</p>
<p>为了降低随机梯度的方差，从而使得迭代算法更加稳定，也为了充分利用高度优化的矩阵运算操作，实际运用中我们会同时处理若干数据，这种方法被称为小批量梯度下降法(Mini-Batch Gradient Descent)。假设我们每次同时处理m个训练数据，则目标函数及其梯度为<br>$$<br>L(\theta)=\frac{1}{m}\sum_{j=1}^m L(f(x_{i_{j}},\theta),y_{i_{j}})<br>$$</p>
<p>$$<br>\nabla L(\theta)=\frac{1}{m}\sum_{j=1}^m\nabla L(f(x_{i_{j}},\theta),y_{i_j})<br>$$</p>
<p>对于小批量梯度下降法的使用，有三点需要注意的：</p>
<p>(1)如何选取参数m？不同应用中，最优的m通常会不一样，需要调参选取，一般选取2的幂次来充分利用矩阵运算操作，可以在2的幂次中挑选最优的取值，例如32、64、128等。</p>
<p>(2)如何挑选m个训练数据？为了避免数据的特定顺序给算法收敛带来的影响，一般会在每次遍历训练数据之前，对所有数据进行随机排序，然后在每次迭代时按顺序挑选m个训练数据直到遍历完所有数据。</p>
<p>(3)如何选取学习速率𝝰？为了加快收敛速率，同时提高求解精度，通常会采用衰减学习速率方案：一开始算法采用较大的学习速率，当误差曲线进入平台期后，减小学习速率做更精细的调整。最优的学习速率方案也通常需要调参才能得到。</p>
<p>综上，通常采用小批量梯度下降法解决训练数据量过大的问题。每次更新模型参数时，只需要处理m个训练数据即可，其中m是一个远小于总数据量M的常数，这样能够大大加快训练过程。</p>
</li>
</ul>
</blockquote>
<h2 id="随机梯度下降法的加速"><a href="#随机梯度下降法的加速" class="headerlink" title="随机梯度下降法的加速"></a>随机梯度下降法的加速</h2><p>随机梯度下降法有时候会成为一个坑，我们这里对它进行一个剖析。</p>
<blockquote>
<ul>
<li><p>随机梯度下降法失效的原因——摸着石头下山。(较容易)</p>
<p>我们经常用下山来形容最优化，随机梯度下降就像是蒙眼下山，而随机梯度下降法就好比蒙着眼睛下山。批量梯度下降法在全部训练集上计算准确的梯度，即<br>$$<br>\sum_{i=1}^n \nabla_\theta f(\theta;x_i,y_i)+\nabla_\theta\phi(\theta)<br>$$<br>其中f是损失函数，𝛗是正则项，而随机梯度则是使用单个样本的梯度来估计当前梯度。，随机梯度最容易出问题的是梯度图中的“山谷”和“鞍点”会导致随机梯度停止不变。</p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>为了改进随机梯度下降，研究者做了哪些改动？提出了哪些方法？(一般)</p>
<p>随机梯度下降法本质上就是使用一个迭代方法更新参数，每次迭代在当前位置基础上进行更新，更新公式为<br>$$<br>\theta_{t+1}=\theta_t-\eta g_t<br>$$<br>改造的随机梯度下降法仍然是基于这个更新公式</p>
<ul>
<li><p>动量(Momentum)方法：</p>
<p>在山谷与鞍点停滞不前的随机梯度下降法，假如能够保持“惯性”，就有机会找到真正的最优方向，因此使用模型方法，模型参数的迭代公式为<br>$$<br>v_t=\gamma v_{t-1}+\eta g_t<br>$$</p>
<p>$$<br>\theta_{t+1}=\theta_t-v_t<br>$$</p>
<p>前进的“步伐”有两部分决定，一部分是常规的学习速率与当前估计梯度的乘积，，二是衰减的前一次步伐，动量方法的更新公式对“惯性”的利用就体现在对前一次“步伐”信息的重利用上。而前一次步伐的衰减系数扮演的是阻力。</p>
<p>相较于随机梯度下降法，动量方法收敛速度更快，收敛曲线更稳定。</p>
</li>
<li><p>AdaGrad方法</p>
<p>除了利用上次步伐的惯性，我们还能获得什么呢？我们期待获得对周围环境的感知，随机梯度下降法对周围环境的感知，是指在参数空间中，根据不同参数一些经验性判断，自适应地确定参数的学习速率，不同参数的更新步幅是不同的。比如文本处理中训练词嵌入模型的参数时，有些词或词组频繁出现，有的词或词组极少出现。数据的稀疏性导致相应参数的梯度的稀疏性，不频繁出现的词或词组参数的梯度在大多数情况下为0，因此它们的学习率很大，而经常更新的参数我们希望它的学习速率能够减小。AdaGrad方法采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏，具体的参数更新公式为<br>$$<br>\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\sum_{k=1}^t g_{k,i}^2 + \epsilon}}g_{t,i}<br>$$<br>其中𝛉<sub>t+1,i</sub>表示t+1时刻的参数向量𝛉<sub>t+1</sub>的第i个参数，g<sub>k,i</sub>表示k时刻的梯度向量g<sub>k</sub>的第i个维度。另外分母中求和的形式实现了退火过程，意味着随着时间推移，学习速率越来越小。</p>
</li>
<li><p>Adam方法</p>
<p>Adam方法将惯性保持和环境感知两个优点集于一身。一方面，Adam记录梯度的一阶矩(first moment)，即过往梯度与当前梯度的平均，这体现惯性保持；另一方面，Adam还记录梯度的二阶矩(second moment)，即过往梯度平方和当前梯度平方的平均，这类似AdaGrad方法，体现了环境感知能力，为不同参数产生自适应的学习速率。一阶矩和二阶矩都类似于滑动窗口内求平均的思想，也就是当前梯度和近一段时间内梯度的平均值，具体来说，他们呢采用指数衰退平均(exponential decay average)技术：<br>$$<br>m_t=\beta_1m_{t-1}+(1-\beta_1)g_t<br>$$</p>
<p>$$<br>v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2<br>$$</p>
<p>一阶矩相当于梯度估计结果的期望，二阶矩相当于从开始到现在梯度估计结果平方的期望，一阶矩绝对值和二阶矩都很大时，说明此时梯度大且稳定；前者小后者大时，说明梯度不稳定；前者大后者小这种情况不可能出现；前者和后者都小说明优化进入了平台期，梯度趋于0。具体来说，Adam的更新公式为<br>$$<br>\theta_{t+1}=\theta_t -\frac{\eta *\hat{m}_t}{\sqrt{\hat{v}_t+\epsilon}}<br>$$<br>其中<br>$$<br>\hat{m}_t=\frac{m_t}{1-\beta_1^t},\hat{v}_t=\frac{v_t}{1-\beta_2^t}<br>$$</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="L1正则化与稀疏性"><a href="#L1正则化与稀疏性" class="headerlink" title="L1正则化与稀疏性"></a>L1正则化与稀疏性</h2><p>稀疏性说白了就是让模型参数很多都为0，这样相当于时做了一次特征选择。</p>
<blockquote>
<ul>
<li><p>L1正则化能够使模型参数具有稀疏性的原理是什么？(一般)</p>
<ul>
<li><p>角度一：解空间形状</p>
<p>我们可以通过二维的情况来找到一个直观的解释，L1正则项约束后的解空间形状是一个菱形，对角边是x轴和y轴，L2正则项约束后的解空间形状是一个圆形。我们的最优化目标函数等高线相较于L2的解空间，很明显更容易和L1的解空间在坐标轴上产生交点，此时我们可以得到稀疏解。</p>
<p>这个解释正确但是不够精细，我们通过KKT条件给出解释，实际上正则项和带约束条件是等价的。我们将最优化问题添加约束为w的L2范数平方不能大于m<br>$$<br>min \sum_{i=1}^N(y_i-w^Tx_i)^2<br>$$</p>
<p>$$<br>s.t. ||w||^2_2≤m<br>$$</p>
<p>为了求解带约束条件的凸优化问题，我们使用拉格朗日法<br>$$<br>\sum_{i=1}^N (y_i-w^Tx_i)^2+\lambda (||w||^2_2-m)<br>$$<br>若w*和λ*分别是原问题和对偶问题的最优解，则根据KKT条件，它们应该满足<br>$$<br>0=\nabla_w(\sum_{i=1}^N(y_i-w^{<em>T}x_i)^2+\lambda^</em>(||w^*||^2_2-m))<br>$$</p>
<p>$$<br>0≤\lambda^*<br>$$</p>
<p>仔细看就会发现，第一个式子就是w<em>为带L2正则项优化问题的最优解的条件，而λ\</em>就是L2正则项前面的正则参数。</p>
<p>这时回头再看开头的问题就清晰了。L2正则化相当于为参数定义了一个圆形的解空间，而L1相当于为参数定义了一个菱形的解空间。</p>
</li>
<li><p>角度二：函数叠加</p>
<p>第二个角度试图用更直观的图示来解释L1产生稀疏性这一现象，仅考虑一维的情况，多维情况是类似的。假设目标函数最小值在非0处产生，考虑加上L2正则化项，目标函数变成了L(w)+cw<sup>2</sup>，最小值点向原点靠近，但仍然非原点；考虑加上L1正则化项，目标函数变成了L(w)+C|w|，此时最小值点在原点，产生了稀疏性。</p>
<p>产生上述现象原因在于，假如L1正则项之后，正则项部分在原点左边导数为-C，在原点右边导数为C，因此只要愿目标函数导数绝对值小于C，那么带正则项目的目标函数在左边递减，右边递增，原点必然是最小值点；而带L2的目标函数只有在原点处导数为0，只要原目标函数在原点处导数不为0，那么最小值点就不会在原点，所以L2只有减小w绝对值的作用，对解空间的稀疏性没有贡献。</p>
<p>在一些在线梯度下降算法中，往往会采用截断梯度法来产生稀疏性，这同L1正则项产生稀疏性的原理是类似的。</p>
</li>
<li><p>角度3:贝叶斯先验</p>
<p>从贝叶斯派的角度来理解L1正则化和L2正则化，简单的解释是，L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于对模型引入了高斯先验，而拉普拉斯先验使得参数为0的概率更大。原因在于两种分布在0处曲线L1是尖峰L2是平滑。</p>
</li>
</ul>
</li>
</ul>
</blockquote>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/05/03/百面机器学习-优化算法/" target="_blank" title="百面机器学习-优化算法">http://xiangweixi.cn/2021/05/03/百面机器学习-优化算法/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/05/08/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%87%87%E6%A0%B7/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          百面机器学习-采样
        
      </div>
    </a>
  
  
    <a href="/2021/04/27/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">百面机器学习-概率图模型</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.</span> <span class="nav-text">有监督学习的损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">2.</span> <span class="nav-text">机器学习中的优化问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">经典优化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E9%AA%8C%E8%AF%81"><span class="nav-number">4.</span> <span class="nav-text">梯度验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">5.</span> <span class="nav-text">随机梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E5%8A%A0%E9%80%9F"><span class="nav-number">6.</span> <span class="nav-text">随机梯度下降法的加速</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7"><span class="nav-number">7.</span> <span class="nav-text">L1正则化与稀疏性</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2022 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright © 2022 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>