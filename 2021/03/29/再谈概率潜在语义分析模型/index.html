<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>再谈概率潜在语义分析模型 | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="unsupervised learning" />
  
  
  
  
  <meta name="description" content="主题模型基于之前的话题模型来表示文本容易导致词义相近的词被识别成为表达不同主题，我们需要使用主题模型来将这些词或词组映射到同一维度上去。《百面机器学习》中，它被划分在概率图模型类别中，它的思想是如果两个词属于一个主题，那么给定那个主题，生成这两个词的概率都应该是比较高的。主题模型干的事情，就是从文本库中发现主题，并计算出每篇文章对应的主题。我们主要介绍概率潜在语义分析(pLSA)以及潜在狄利克雷分">
<meta property="og:type" content="article">
<meta property="og:title" content="再谈概率潜在语义分析模型">
<meta property="og:url" content="http://example.com/2021/03/29/%E5%86%8D%E8%B0%88%E6%A6%82%E7%8E%87%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="主题模型基于之前的话题模型来表示文本容易导致词义相近的词被识别成为表达不同主题，我们需要使用主题模型来将这些词或词组映射到同一维度上去。《百面机器学习》中，它被划分在概率图模型类别中，它的思想是如果两个词属于一个主题，那么给定那个主题，生成这两个词的概率都应该是比较高的。主题模型干的事情，就是从文本库中发现主题，并计算出每篇文章对应的主题。我们主要介绍概率潜在语义分析(pLSA)以及潜在狄利克雷分">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.tomtung.com/images/2011-10-19-plsa_graph.png">
<meta property="og:image" content="https://www.researchgate.net/profile/Kevin_Gimpel/publication/242385158/figure/download/fig5/AS:646814017519616@1531223887002/The-latent-Dirichlet-allocation-LDA-model-There-are-M-documents-and-each-document.png">
<meta property="article:published_time" content="2021-03-29T12:40:04.000Z">
<meta property="article:modified_time" content="2021-04-07T01:23:51.526Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="unsupervised learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.tomtung.com/images/2011-10-19-plsa_graph.png">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-再谈概率潜在语义分析模型" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      再谈概率潜在语义分析模型
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/03/29/%E5%86%8D%E8%B0%88%E6%A6%82%E7%8E%87%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/" class="article-date">
	  <time datetime="2021-03-29T12:40:04.000Z" itemprop="datePublished">2021-03-29</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="主题模型"><a href="#主题模型" class="headerlink" title="主题模型"></a>主题模型</h2><p>基于之前的话题模型来表示文本容易导致词义相近的词被识别成为表达不同主题，我们需要使用主题模型来将这些词或词组映射到同一维度上去。《百面机器学习》中，它被划分在概率图模型类别中，它的思想是如果两个词属于一个主题，那么给定那个主题，生成这两个词的概率都应该是比较高的。主题模型干的事情，就是从文本库中发现主题，并计算出每篇文章对应的主题。<br>我们主要介绍概率潜在语义分析(pLSA)以及潜在狄利克雷分布(Latent Dirichlet Allocation)</p>
<h2 id="概率潜在语义分析"><a href="#概率潜在语义分析" class="headerlink" title="概率潜在语义分析"></a>概率潜在语义分析</h2><p>概率潜在语义分析将文本表示为文本-单词共现数据，模型中将话题作为隐藏变量，将文本和单词作为可观测变量，它的具体表现为两种子模型，生成模型和共现模型。</p>
<h3 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h3><p>单词集合W有M个单词，文本集合D有N个文本，话题集合Z有K个话题，于是生成模型为从文本生成话题再生成单词的形式，即：<br>$$<br>P(T)=\mathop{\prod}\limits_{(w,d)}P(w,d)^{n(w,d)}<br>$$<br>n(w,d)表示的是单词-文本对(w,d)出现的次数，每个单词-文本对的生成概率为：<br>$$<br>P(w,d)=P(d)P(w|d)<br>$$<br>$$<br>=P(d)\sum_z P(w,z|d)<br>$$<br>$$<br>=P(d)\sum_z P(z|d)P(w|z)<br>$$</p>
<p>生成模型是一个概率有向图模型。</p>
<p><img src="http://blog.tomtung.com/images/2011-10-19-plsa_graph.png" alt="image0"></p>
<h3 id="共现模型"><a href="#共现模型" class="headerlink" title="共现模型"></a>共现模型</h3><p>共现模型在概率公式上是等价的：<br>$$<br>P(w,d)=\sum_{z\in Z}P(z)P(w|z)P(d|z)<br>$$<br>但是它们的模式不同，这也导致它们的学习算法形式不同。我们可以将共现模型表述为潜在语义分析中截断奇异值分解的形式，从左到右的矩阵表示的概率分别为P(w|z),P(z),P(d|z)。</p>
<p>由于模型中包含了隐藏变量，我们使用EM算法进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PLSA</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,text_list,k</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param text_list:以列表形式表示的文本</span></span><br><span class="line"><span class="string">        :param k:话题数量</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.text_list = text_list</span><br><span class="line">        self.text_num = <span class="built_in">len</span>(text_list)</span><br><span class="line">        self.get_X()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_X</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.cuted_text = [jieba.lcut(text,cut_all=<span class="literal">True</span>) <span class="keyword">for</span> text <span class="keyword">in</span> self.text_list]</span><br><span class="line">        self.word_all = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.cuted_text:</span><br><span class="line">            self.word_all.extend(i)</span><br><span class="line">        self.word_set = <span class="built_in">list</span>(<span class="built_in">set</span>(self.word_all))</span><br><span class="line">        self.word_num = <span class="built_in">len</span>(self.word_set)</span><br><span class="line">        self.word_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> index,word <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.word_set):</span><br><span class="line">            self.word_dict[word] = index</span><br><span class="line">        self.X = np.zeros((self.word_num,self.text_num))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.text_num):</span><br><span class="line">            count_ = collections.Counter(self.cuted_text[i])</span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> count_.items():</span><br><span class="line">                self.X[self.word_dict[k],i] = v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_p_z_wd</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.z_wd  = np.zeros((self.word_num,self.text_num,self.k))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.word_num):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.text_num):</span><br><span class="line">                self.z_wd[i,j] = np.array([self.w_z[i]*self.z_d[:,j]]) / np.<span class="built_in">sum</span>([self.w_z[i]*self.z_d[:,j]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self,max_iter</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param max_iter:EM算法迭代次数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.w_z  = np.random.random((self.word_num,self.k))</span><br><span class="line">        self.z_d = np.random.random((self.k,self.text_num))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            self.update_p_z_wd()</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.word_num):</span><br><span class="line">                    self.w_z[i,k] = np.<span class="built_in">sum</span>(self.X[i]*self.z_wd[i,:,k])/np.<span class="built_in">sum</span>(self.X*self.z_wd[:,:,k])</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.text_num):</span><br><span class="line">                    self.z_d[k,j] = np.<span class="built_in">sum</span>(self.X[:,j]*self.z_wd[:,j,k])/np.<span class="built_in">sum</span>(self.X[:,j])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    text_list = [</span><br><span class="line">    <span class="string">&#x27;一个月前，足协杯十六进八的比赛，辽足费尽周折对调主客场，目的只是为了葫芦岛体育场的启用仪式。那场球辽足5比0痛宰“主力休息”的天津泰达。几天后中超联赛辽足客场对天津，轮到辽足“全替补”，\</span></span><br><span class="line"><span class="string">    1比3输球，甘为天津泰达保级的祭品。那时，辽足以“联赛保级问题不大，足协杯拼一拼”作为主力和外援联赛全部缺阵的理由。&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;被一脚踹进“忘恩负义”坑里的孙杨，刚刚爬出来，又有手伸出来，要把孙杨再往坑里推。即使是陪伴孙杨参加世锦赛的张亚东(微博)教练，\</span></span><br><span class="line"><span class="string">    也没敢大义凛然地伸出援手，“孙杨愿意回去我不拦”，球又踢给了孙杨。张亚东教练怕什么呢？&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;孙杨成绩的利益分配，以及荣誉的分享，圈里人都知道，拿了世界冠军和全运冠军，运动员都会有相应的高额奖金，那么主管教练也会得到与之对应的丰厚奖励，\</span></span><br><span class="line"><span class="string">    所以孙杨获得的荣誉，也会惠及主管教练。&#x27;</span>]</span><br><span class="line">    lsa = PLSA(text_list,k=<span class="number">2</span>)</span><br><span class="line">    lsa.fit(<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># print(lsa.w_z)</span></span><br><span class="line">    <span class="comment"># print(lsa.z_d)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="潜在狄利克雷分布"><a href="#潜在狄利克雷分布" class="headerlink" title="潜在狄利克雷分布"></a>潜在狄利克雷分布</h2><p>《百面机器学习》中，将LDA评价为pLSA的贝叶斯版本，从这里你也可以得到pLSA采用的是频率学派的思想，从之前我们学习的内容也体现出了频率学派对于概率的看法：将文章的主题分布P(z|d)以及主题对应的词分布P(w|z)都作为未知的常数进行求解；而LDA的思想则是贝叶斯学派对概率的观点：一个服从先验概率分布的随机变量。在LDA中，模型的先验分布和后验分布都被预设为服从狄利克雷分布。其中有两个超参数用于修正先验与后验中的狄利克雷分布。说了这么多，先来谈谈最基本的分布定义。</p>
<h3 id="分布定义"><a href="#分布定义" class="headerlink" title="分布定义"></a>分布定义</h3><p>先说多项分布（multinomial distribution），它是一种多元离散随机变量的概率分布，属于之前学习的二项分布的扩展；假设进行n词独立随机试验，每次试验都有可能出现k种结果，p<sub>i</sub>为第i个结果出现的概率，n<sub>i</sub>为第i种结果出现的次数，X代表的是所有可能结果出现的次数，X<sub>i</sub>表示第i种结果出现的次数，这就说随机变量X服从多项分布，记作X~Multi(n,p)。<br>概率质量函数为<br>$$<br>P(X)=P(X_1=n_1,X_2=n_2,…,X_k=n_k)=\frac{n!}{n_1!n_2!…n_k!}p_1^{n_1}p_2^{n_2}…p_k^{n_k}<br>$$<br>$$<br>=\frac{n!}{\mathop{\prod}\limits_{i=1}^{k}n!}\mathop{\prod}\limits_{i=1}^{k}p_i^{n_i}<br>$$<br>$$<br>\sum_{i=1}^k p_i=1,\sum_{i=1}^k n_i=n.<br>$$<br>接下来就是狄利克雷分布，它是一种多元连续型随机变量的概率分布，也被称为多元贝塔分布，从这个名字就能知道它是贝塔分布的拓展。在机器学习的贝叶斯相关方法中，狄利克雷分布常被作为多项分布的先验分布使用，它的密度函数可以被表示为：<br>$$<br>p(\theta|\alpha)=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\prod_{i=1}^{k}\Gamma(\alpha_i)}\prod_{i=1}^k \theta_i^{\alpha_i-1}<br>$$</p>
<p>$$<br>\sum_{i=1}^k \theta_i=1,\theta_i≥0,\alpha={\alpha_1,\alpha_2,…,\alpha_k},\alpha_i&gt;0.<br>$$</p>
<p>$$<br>\Gamma(s)=\int_0^\infin x^{s-1}e^{-x}dx,s&gt;0<br>$$</p>
<p>记作𝛉~Dir(𝝰)。令<br>$$<br>B(\alpha)=\frac{\prod_{i=1}^{k}\Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^k\alpha_i)}<br>$$<br>这就是我们所说的拓展的贝塔函数，也叫多元贝塔函数。</p>
<p>最后我们定义二项分布和贝塔分布，二项分布没啥多说的，定义为<br>$$<br>P(X=m)=C_n^mp^m(1-p)^{n-m}<br>$$<br>而贝塔分布指的是当X为连续随机变量，取值范围为[0,1]，概率密度函数为<br>$$<br>p(x)=\left{\begin{matrix}\frac{1}{B(s,t)}x^{s-1}(1-x)^{t-1},0≤x≤1<br>\0,其他\end{matrix}\right.<br>$$</p>
<p>$$<br>B(s,t)=\frac{\Gamma(s)\Gamma(t)}{\Gamma(s+t)}<br>$$</p>
<p>共轭先验是指先验分布和后验分布同属一类时，先验分布和后验分布被称为共轭分布(conjugate distributions)，先验分布被称为共轭先验(conjugate distributions)。使用共轭分布的好处便是从先验分布计算后验分布。<br>之前已经给出了p(𝛉|𝝰)的计算公式，根据贝叶斯规则，在给定样本数据D和参数𝝰条件下，𝛉的后验概率分布为：<br>$$<br>p(\theta|D,\alpha)=\frac{p(D|\theta)p(\theta|\alpha)}{p(D|\alpha)}<br>$$</p>
<p>$$<br>=\frac{\prod_{i=1}^k \theta^{n_i}\frac{1}{B(\alpha)}\theta_i^{\alpha_i-1}}{\int\prod_{i=1}^k \theta^{n_i}\frac{1}{B(\alpha)}\theta_i^{\alpha_i-1}d\theta}<br>$$</p>
<p>$$<br>=\frac{1}{B(\alpha+n)}\prod_{i=1}^k \theta_i^{\alpha_i+n_i-1}<br>$$</p>
<p>$$<br>=Dir(\theta|\alpha+n)<br>$$</p>
<h3 id="LDA模型"><a href="#LDA模型" class="headerlink" title="LDA模型"></a>LDA模型</h3><p>LDA模型的思想是文本集合的自动生成过程。可以将LDA视作pLSA的贝叶斯拓展，它们都将单词视作话题的多项分布，将文本视作单词的多项分布，不同点在于pLSA不认为主题分布以及词分布服从先验分布（或者说认为它们服从均匀分布）；LDA基于贝叶斯学习进行参数估计，而pLSA根据EM算法进行学习。LDA使用先验概率分布，可以有效防止过拟合。</p>
<p>LDA使用三个集合：v个单词的集合w，n个单词集合的文本集合w<sub>m</sub>，k个话题的集合Z；每个话题由条件概率p(w|z)决定，这就是话题的单词分布，参数𝛗服从狄利克雷分布，超参数为𝛃；每个文本由条件概率p(z|w<sub>m</sub>)决定，这是文本的话题分布，参数𝛉服从狄利克雷分布，超参数为𝝰。</p>
<p>算法具体如下：<br>(1)对于话题z：生成参数𝛗~Dir(𝛃)，作为话题的单词分布p(w|z);<br>(2)对于文本w<sub>m</sub>：生成参数𝛉~Dir(𝝰)，作为文本的话题分布p(z|w<sub>m</sub>);<br>(3)之后对于文本中的特定位置单词先根据Mult(𝛉)生成话题，再根据Mult(𝛗)生成单词；</p>
<img src="https://www.researchgate.net/profile/Kevin_Gimpel/publication/242385158/figure/download/fig5/AS:646814017519616@1531223887002/The-latent-Dirichlet-allocation-LDA-model-There-are-M-documents-and-each-document.png" alt="LDA的概率图模型" style="zoom:50%;">

<p>空心圆全部表示隐变量。</p>
<h3 id="LDA的吉布斯抽样算法"><a href="#LDA的吉布斯抽样算法" class="headerlink" title="LDA的吉布斯抽样算法"></a>LDA的吉布斯抽样算法</h3><p>LDA的学习很难精确求解，我们常用的LDA近似求解方法有吉布斯抽样(Gibbs sampling)和变分推理(variational inference)。吉布斯抽样的优点是实现简单，缺点是迭代次数可能过多。<br>LDA的学习目标是对联合概率分布p(w,z,𝛉,𝛗|𝝰,𝛃)进行估计，其中z,𝛉,𝛗是隐变量，w时间观测变量，𝝰,𝛃是两个狄利克雷分布的超参数；<br>这里介绍一下吉布斯采样，它是MCMC方法中Metropolis-Hastings算法的一个特例，它的基本思想就是每次只对样本中的一个维度进行更新，其他维度的分量进行固定，若目标分布为p(x)，样本为x={x<sub>1</sub>,x<sub>2</sub>,…,x<sub>d</sub>}，采样过程如下：<br>(1)随机初始化初始状态x<sup>(0)</sup>={x<sub>1</sub><sup>(0)</sup>,x<sub>2</sub><sup>(0)</sup>,…,x<sub>d</sub><sup>(0)</sup>};<br>(2)进行迭代：对于第t步前一次迭代产生的样本x<sup>(t-1)</sup>=(x<sub>1</sub><sup>(t-1)</sup>,x<sub>2</sub><sup>(t-1)</sup>,…,x<sub>d</sub><sup>(t-1)</sup>)，依次采样与更新每个维度的值，依次抽取分量x<sub>1</sub><sup>(t)</sup>~p(x<sub>1</sub><sup>(t)</sup>|x<sub>2</sub><sup>(t-1)</sup>,x<sub>3</sub><sup>(t-1)</sup>…,x<sub>d</sub><sup>(t-1)</sup>),x<sub>2</sub><sup>(t)</sup>~p(x<sub>2</sub><sup>(t)</sup>|x<sub>1</sub><sup>(t-1)</sup>,x<sub>3</sub><sup>(t-1)</sup>…,x<sub>d</sub><sup>(t-1)</sup>),…,x<sub>d</sub><sup>(t)</sup>~p(x<sub>d</sub><sup>(t)</sup>|x<sub>1</sub><sup>(t-1)</sup>,x<sub>2</sub><sup>(t-1)</sup>…,x<sub>d-1</sub><sup>(t-1)</sup>);<br>(3)得到新的样本x<sup>(t)</sup>={x<sub>1</sub><su>(t),x<sub>2</sub><sup>(t)</sup>,…,x<sub>d</sub><sup>(t)</sup>}；<br>采集的样本需要舍弃“燃烧期”；<br>LDA模型的学习通常采用收缩吉布斯抽样方法(collapsed Gibbs sampling)方法，即通过对隐变量𝛉和𝛗的积分，将待估计概率转换为p(w,z|𝝰,𝛃)，其中w是可观测的，变量z是不可观测的，对后验概率p(z|w,𝝰,𝛃)进行吉布斯抽样，得到分布p(z|w,𝝰,𝛃)的样本集合；再利用它对参数𝛉和𝛗进行估计，最终可以得到p(w,z,𝛉,𝛗|𝝰,𝛃)的所有参数估计值。</su></p>
<p>LDA吉布斯抽样算法步骤如下：<br>输入：文本单词序列w={w<sub>1</sub>,…,w<sub>m</sub>,…,w<sub>M</sub>},w<sub>m</sub>={w<sub>m1</sub>,…,w<sub>mn</sub>,…,w<sub>mN<sub>m</sub></sub>}；<br>输出：文本话题序列z={z<sub>1</sub>,…,z<sub>m</sub>,…,z<sub>M</sub>},z<sub>m</sub>={z<sub>m1</sub>,…,z<sub>mn</sub>,…,z<sub>mN<sub>m</sub></sub>}的后验概率分布p(z|w,𝝰,𝛃)的样本计数，模型参数𝛉与𝛗的估计值；<br>参数：超参数𝝰和𝛃，话题个数K；<br>(1)设所有计数矩阵的元素n<sub>mk</sub>,n<sub>kv</sub>，计数向量的元素n<sub>m</sub>,n<sub>k</sub>初值为0；n<sub>kv</sub>是第k个话题中第v个单词除开当前单词的计数，n<sub>mk</sub>是第m个文本中第k个话题除开当前话题的计数；<br>(2)对所有文本w<sub>m</sub>，m=1,2,…,M，对第m个文本中所有单词w<sub>mn</sub>：抽样话题z<sub>mn</sub>=z<sub>k</sub>~Mult(1/K)；增加文本-话题n<sub>mk</sub>,文本-话题和n<sub>m</sub>,话题-单词n<sub>kv</sub>,话题-单词和n<sub>k</sub>计数；<br>(3)循环进行下列操作，直到进入“燃烧期”：对所有文本w<sub>m</sub>，对第m个文本中的所有单词w<sub>mn</sub>，当前w<sub>mn</sub>是第v个单词，话题指派z<sub>mn</sub>是第k个话题；减少计数n<sub>mk</sub>,n<sub>m<sub>,n<sub>vk</sub>,n<sub>k</sub>；再按照满条件分布进行抽样<br>$$<br>p(z_i|z_{-i},w,\alpha,\beta)\propto \frac{n_{kv}+\beta_v}{\sum_{v=1}^V (n_{kv}+\beta_v)}.\frac{n_{mk}+\alpha_k}{\sum_{k=1}^K(n_{mk}+\alpha_k)}<br>$$<br>z<sub>-i</sub>是指z中除了第i个话题之外的所有话题；得到新的第k’个话题并分配给z<sub>mn</sub>后；增加n<sub>k’</sub>,n<sub>vk’</sub>,n<sub>mk</sub>,n<sub>m</sub>计数得到更新的两个计数矩阵n<sub>kv</sub>,n<sub>mk</sub>，表示后验概率分布p(z|w,𝝰,𝛃)的样本计数；<br>(4)利用得到的样本计数，估计模型参数：<br>$$<br>\theta_{mk}=\frac{n_{mk}+\alpha_k}{\sum_{k=1}^K(n_{mk}+\alpha_k)}<br>$$</sub></sub></p>
<p>$$<br>\varphi =\frac{n_{kv}+\beta_v}{\sum_{v=1}^V (n_{kv}+\beta_v)}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LDAGibbs</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, K=<span class="number">3</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        利用collapsed gibbs sampling方法实现LDA</span></span><br><span class="line"><span class="string">        :param K: 主题个数，默认值为3</span></span><br><span class="line"><span class="string">        :param V: 总共单词个数</span></span><br><span class="line"><span class="string">        :param M: 文本个数</span></span><br><span class="line"><span class="string">        :param tockens: 单词tockens</span></span><br><span class="line"><span class="string">        :param alpha: theta的超参数，维度(K,)</span></span><br><span class="line"><span class="string">        :param theta: 文本主题矩阵，服从狄利克雷分布，维度(M,K)</span></span><br><span class="line"><span class="string">        :param beta: varphi的超参数，维度(V,)</span></span><br><span class="line"><span class="string">        :param varphi: 单词主题矩阵，服从狄利克雷分布，维度(K,V)</span></span><br><span class="line"><span class="string">        :param z: 话题集合，维度(M,Nm),Nm表示第m个文本的单词个数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.K = K</span><br><span class="line">        self.V = <span class="literal">None</span></span><br><span class="line">        self.M = <span class="literal">None</span></span><br><span class="line">        self.tokens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.params = &#123;</span><br><span class="line">            <span class="string">&#x27;alpha&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;theta&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;beta&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;varphi&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;z&#x27;</span>: <span class="literal">None</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_params</span>(<span class="params">self, texts</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化参数，除了上面定义的参数，公式中统计的参数也在这儿初始化</span></span><br><span class="line"><span class="string">        :param texts: 输入文本</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        alpha = np.ones(self.K)</span><br><span class="line">        <span class="comment"># 文档主题参数先占位，最后再进行更新</span></span><br><span class="line">        theta = np.ones((self.M ,self.K))</span><br><span class="line">        beta = np.ones(self.V)</span><br><span class="line">        <span class="comment"># 单词主题参数也先占位，最后再进行更新</span></span><br><span class="line">        varphi = np.ones((self.K, self.V))</span><br><span class="line">        z = []</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">            N = <span class="built_in">len</span>(texts[m])</span><br><span class="line">            z.append(np.zeros(N))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 代表第k个主题的第v个单词个数</span></span><br><span class="line">        self.n_kv = np.zeros((self.K, self.V))</span><br><span class="line">        <span class="comment"># 代表第k个主题有多少个单词</span></span><br><span class="line">        self.n_k = np.zeros(self.K)</span><br><span class="line">        <span class="comment"># 代表第m个文档第k个主题的单词个数</span></span><br><span class="line">        self.n_mk = np.zeros((self.M, self.K))</span><br><span class="line">        <span class="comment"># 每一个元素代表第m个文档有多少个单词</span></span><br><span class="line">        self.n_m = np.zeros(self.M)</span><br><span class="line"></span><br><span class="line">        z = np.array(z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化中做一次统计，且对z随机初始化</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">            N = <span class="built_in">len</span>(texts[m])</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                rand_topic = <span class="built_in">int</span>(np.random.randint(<span class="number">0</span>, self.K))</span><br><span class="line">                z[m][v] = rand_topic</span><br><span class="line">                self.n_kv[rand_topic][<span class="built_in">int</span>(texts[m][v])] += <span class="number">1</span></span><br><span class="line">                self.n_k[rand_topic] += <span class="number">1</span></span><br><span class="line">                self.n_mk[m][rand_topic] += <span class="number">1</span></span><br><span class="line">            self.n_m[m] = N</span><br><span class="line"></span><br><span class="line">        self.params = &#123;</span><br><span class="line">            <span class="string">&#x27;alpha&#x27;</span>: alpha,</span><br><span class="line">            <span class="string">&#x27;theta&#x27;</span>: theta,</span><br><span class="line">            <span class="string">&#x27;beta&#x27;</span>: beta,</span><br><span class="line">            <span class="string">&#x27;varphi&#x27;</span>: varphi,</span><br><span class="line">            <span class="string">&#x27;z&#x27;</span>: z</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sample_topic</span>(<span class="params">self, m, v, texts</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算z_mv对应主题的概率值，并通过概率值进行多项分布采样，得到当前的主题</span></span><br><span class="line"><span class="string">        math:</span></span><br><span class="line"><span class="string">        p(z):</span></span><br><span class="line"><span class="string">        $$p(z_&#123;mv&#125; | z_&#123;-mv&#125;, w , alpha, beta) =  frac &#123;n_&#123;kv&#125; +beta_v -1&#125; &#123;sum_&#123;v=1&#125;^V (n_&#123;kv&#125; + beta_v) -1&#125; .</span></span><br><span class="line"><span class="string">        frac &#123;n_&#123;mk&#125; + alpha_k - 1&#125; &#123;sum_&#123;k=1&#125;^K (n_&#123;mk&#125; + alpha_k) - 1&#125;$$</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param m: 表示调用的时候，上层函数计算的是第m个文档</span></span><br><span class="line"><span class="string">        :param v: 第v个单词</span></span><br><span class="line"><span class="string">        :param texts: 语料</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 首先是排除当前的单词z[m][v]</span></span><br><span class="line">        old_topic = <span class="built_in">int</span>(self.params[<span class="string">&#x27;z&#x27;</span>][m][v])</span><br><span class="line">        self.n_kv[old_topic][<span class="built_in">int</span>(texts[m][v])] -= <span class="number">1</span></span><br><span class="line">        self.n_k[old_topic] -= <span class="number">1</span></span><br><span class="line">        self.n_mk[m][old_topic] -= <span class="number">1</span></span><br><span class="line">        self.n_m[m] -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 依次计算该单词的p(z_mv=k | *)</span></span><br><span class="line">        p = np.zeros(self.K)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K):</span><br><span class="line">            p[k] = (self.n_mk[m][k] + self.params[<span class="string">&#x27;alpha&#x27;</span>][k]) / (self.n_k[k] + np.<span class="built_in">sum</span>(self.params[<span class="string">&#x27;beta&#x27;</span>])) * \</span><br><span class="line">                   (self.n_kv[k][<span class="built_in">int</span>(texts[m][v])] + self.params[<span class="string">&#x27;beta&#x27;</span>][<span class="built_in">int</span>(texts[m][v])]) \</span><br><span class="line">                   / (self.n_k[k] + np.<span class="built_in">sum</span>(self.params[<span class="string">&#x27;beta&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对概率进行归一化处理</span></span><br><span class="line">        p = p / np.<span class="built_in">sum</span>(p)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 抽样新的主题</span></span><br><span class="line">        new_topic = np.argmax(np.random.multinomial(<span class="number">1</span>, p))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新统计值</span></span><br><span class="line">        self.n_kv[new_topic][<span class="built_in">int</span>(texts[m][v])] += <span class="number">1</span></span><br><span class="line">        self.n_k[new_topic] += <span class="number">1</span></span><br><span class="line">        self.n_mk[m][new_topic] += <span class="number">1</span></span><br><span class="line">        self.n_m[m] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_topic</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collapse_gibbs_sampling</span>(<span class="params">self, texts, max_iter</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        吉布斯采样的循环入口</span></span><br><span class="line"><span class="string">        :param texts: 语料</span></span><br><span class="line"><span class="string">        :param max_iter: 最大循环次数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            print(<span class="string">&#x27;iter: &#x27;</span>, <span class="built_in">iter</span> + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">                N = <span class="built_in">len</span>(texts[m])</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                    topic = self._sample_topic(m, v, texts)</span><br><span class="line">                    self.params[<span class="string">&#x27;z&#x27;</span>][m][v] = topic</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新参数</span></span><br><span class="line"><span class="string">        math:</span></span><br><span class="line"><span class="string">        theta:</span></span><br><span class="line"><span class="string">        $$theta_&#123;mk&#125; = frac &#123;n_&#123;mk&#125; + alpha_k - 1&#125; &#123;sum_&#123;k=1&#125;^K (n_&#123;mk&#125; + alpha_k) - 1&#125;$$</span></span><br><span class="line"><span class="string">        varphi:</span></span><br><span class="line"><span class="string">        $$varphi_&#123;kv&#125; = frac &#123;n_&#123;kv&#125; + beta_v -1&#125; &#123;sum_&#123;v=1&#125;^V (n_&#123;kv&#125; + beta_v) -1&#125; $$</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        但是一般都没有减一，是因为在统计过程中，提前减一了</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 依据统计值，更新单词主题矩阵和文档主题矩阵</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K):</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(self.V):</span><br><span class="line">                self.params[<span class="string">&#x27;varphi&#x27;</span>][k][v] = \</span><br><span class="line">                    (self.n_kv[k][v] + self.params[<span class="string">&#x27;beta&#x27;</span>][v]) / \</span><br><span class="line">                    (self.n_k[k] + np.<span class="built_in">sum</span>(self.params[<span class="string">&#x27;beta&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K):</span><br><span class="line">                self.params[<span class="string">&#x27;theta&#x27;</span>][m][k] = \</span><br><span class="line">                    (self.n_mk[m][k] + self.params[<span class="string">&#x27;alpha&#x27;</span>][k]) / \</span><br><span class="line">                    (self.n_m[m] + np.<span class="built_in">sum</span>(self.params[<span class="string">&#x27;alpha&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, texts, tokens, max_iter=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        模型入口</span></span><br><span class="line"><span class="string">        :param texts: 语料</span></span><br><span class="line"><span class="string">        :param tokens: 单词tokens</span></span><br><span class="line"><span class="string">        :param max_iter: 最大迭代次数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.M = <span class="built_in">len</span>(texts)</span><br><span class="line">        self.V = <span class="built_in">len</span>(tokens)</span><br><span class="line">        self.tokens = tokens</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        self._init_params(texts)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执行程序，统计迭代统计值</span></span><br><span class="line">        self.collapse_gibbs_sampling(texts, max_iter)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 由于统计值一直迭代，最后赋值到模型参数中即可</span></span><br><span class="line">        self._update_params()</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_corpus</span>():</span></span><br><span class="line">    <span class="comment"># 这个过程就是LDA的生成过程</span></span><br><span class="line">    <span class="comment"># 创建一些数据集</span></span><br><span class="line">    M = <span class="number">300</span>  <span class="comment"># 创建的文本数量</span></span><br><span class="line">    K = <span class="number">10</span>   <span class="comment"># topic 数目</span></span><br><span class="line">    V = <span class="number">30</span>   <span class="comment"># 总共单词个数</span></span><br><span class="line">    N = np.random.randint(<span class="number">150</span>, <span class="number">200</span>, size=M)  <span class="comment"># 每个文档的单词数量在150-200</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 文本主题的狄利克雷参数</span></span><br><span class="line">    alpha1 = np.array((<span class="number">20</span>, <span class="number">15</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    alpha2 = np.array((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    alpha3 = np.array((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">18</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单词主题的狄利克雷参数</span></span><br><span class="line">    <span class="comment"># beta = (</span></span><br><span class="line">    <span class="comment">#     np.ones((K, V)) + np.eye(V)[:K] * 19</span></span><br><span class="line">    <span class="comment"># )</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    beta = (</span><br><span class="line">        np.ones((K, V)) + np.array([np.arange(V) % K == t <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(K)]) * <span class="number">19</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单词主题概率</span></span><br><span class="line">    varphi = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: np.random.dirichlet(x), beta)))</span><br><span class="line"></span><br><span class="line">    corpus = []</span><br><span class="line">    theta = np.empty((M, K))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环生成M个文档</span></span><br><span class="line">    <span class="keyword">for</span> M <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="comment"># 生成文档主题概率</span></span><br><span class="line">        <span class="comment"># 前三分之一的文档服从alpha1</span></span><br><span class="line">        <span class="keyword">if</span> M &lt; (M / <span class="number">3</span>):</span><br><span class="line">            theta[M, :] = np.random.dirichlet(alpha1, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">elif</span> M &lt; <span class="number">2</span> * (M / <span class="number">3</span>):</span><br><span class="line">            theta[M, :] = np.random.dirichlet(alpha2, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            theta[M, :] = np.random.dirichlet(alpha3, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        doc = np.array([])</span><br><span class="line">        <span class="comment"># 循环生成每一个文档的单词</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N[M]):</span><br><span class="line">            <span class="comment"># 首先通过文档主题概率选择第M个文档的主题K</span></span><br><span class="line">            z_n = np.random.choice(np.arange(K), p=theta[M, :])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 已经知道主题K，再通过单词主题概率生成单词V</span></span><br><span class="line">            w_n = np.random.choice(np.arange(V), p=varphi[z_n, :])</span><br><span class="line">            doc = np.append(doc, w_n)</span><br><span class="line"></span><br><span class="line">        corpus.append(doc)</span><br><span class="line">    <span class="keyword">return</span> corpus, K, V, varphi, theta</span><br></pre></td></tr></table></figure>
<h3 id="LDA的变分EM法"><a href="#LDA的变分EM法" class="headerlink" title="LDA的变分EM法"></a>LDA的变分EM法</h3><p>先介绍KL散度，KL散度又被称为信息散度或相对熵，是两个概率分布间差异的非对称性度量。相对熵等驾驭两个概率分布的信息熵的差值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差。<br>$$<br>D_{KL}(p||q)=\sum_{i=1}^N[p(x_i)logp(x_i)-p(x_i)logq(x_i)]<br>$$<br>近似分布q(z)与后验分布p(z|x)的KL散度为：<br>$$<br>D(q(z)||p(z|x))=E_q[logq(z)]-E_q[log(p(z|x))]<br>$$<br>$$<br>=E_q[logq(z)]-E_q[log(p(z,x))]+logp(x)≥0<br>$$<br>所以可以得到<br>$$<br>logp(x)≥E_q[log(p(z,x))]-E_q[logq(z)]<br>$$<br>不等式左端被称为证据，右端被称为证据下界(evidence lower bound,ELBO)。变分推理以下步骤：定义变分分布（近似分布）p(x)；推导其下界表达式；最优化证据下界得到证据的最优分布，作为后验分布的近似；<br>为了便于算法计算，引入平均场概念：<br>$$<br>q(z)=\prod_{i=1}^nq(z_i)<br>$$<br>变分推理的任务即是在有观测变量x，隐变量z，参数𝛉的条件下，对参数𝛉进行估计，我们通过定义证据下界：<br>$$<br>L(q,\theta)=E_q[logp(z,x|\theta)]-E_q[logq(z)]<br>$$<br>通过对证据下界的最大化，我们就能得到最优的证据。<br>(1)E步：固定𝛉，求L(q,𝛉)对q的最大化；(2)M步：固定q，求L(q,𝛉)对𝛉的最大化；<br>迭代进行EM直到收敛。<br>对于LDA的变分EM算法，则是<br>输入：给定文本集合D；<br>输出：变分参数以及模型参数；<br>E步：固定模型参数，对证据下界进行最大化求变分参数；<br>M步：固定变分参数，对证据下界进行最大化求模型参数；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> digamma, polygamma, gammaln</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LDAEM</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, K=<span class="number">3</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        变分EM算法实现LDA模型</span></span><br><span class="line"><span class="string">        :param K: 主题个数，默认值为3</span></span><br><span class="line"><span class="string">        :param M: 文档个数</span></span><br><span class="line"><span class="string">        :param Nm: 第m个文档的单词个数</span></span><br><span class="line"><span class="string">        :param N: 文本单词个数列表，(M,)</span></span><br><span class="line"><span class="string">        :param V: 单词集合的个数</span></span><br><span class="line"><span class="string">        :param tockens: 单词tockens</span></span><br><span class="line"><span class="string">        :param gamma: 变分参数，对应文档主题，(M, K)</span></span><br><span class="line"><span class="string">        :param rho: 变分参数，对应单词主题，(K, V)</span></span><br><span class="line"><span class="string">        :param eta: 变分参数，对应文本中单词的主题(M, Nm, K)</span></span><br><span class="line"><span class="string">        :param alpha: 模型参数文档主题theta的参数，(K,)</span></span><br><span class="line"><span class="string">        :param beta: 模型参数单词主题varphi的参数，(V,)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.K = K</span><br><span class="line"></span><br><span class="line">        self.M = <span class="literal">None</span></span><br><span class="line">        self.V = <span class="literal">None</span></span><br><span class="line">        self.tockens = <span class="literal">None</span></span><br><span class="line">        self.N = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.params = &#123;</span><br><span class="line">            <span class="string">&#x27;gamma&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;rho&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;eta&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;alpha&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&#x27;beta&#x27;</span>: <span class="literal">None</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 初始化变分参数</span></span><br><span class="line">        <span class="comment"># 约束条件sum_k gamma_mk = 1, 即对于第m个文档主题之和为1</span></span><br><span class="line">        gamma = np.random.dirichlet(<span class="number">100</span> * np.ones(self.K), self.M)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 约束条件sum_v rho_kv = 1，即对于第k个主题，所有单词之和为1</span></span><br><span class="line">        rho = np.random.dirichlet(<span class="number">100</span> * np.ones(self.V), self.K)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 约束条件 sum_k eta_mnk=1，即对于第m个文档，第n个单词，文档主题概率之和为1</span></span><br><span class="line">        eta = np.array([np.random.dirichlet(<span class="number">100</span> * np.ones(self.K), Nm) <span class="keyword">for</span> Nm <span class="keyword">in</span> self.N])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化模型参数</span></span><br><span class="line">        alpha = np.ones(self.K)</span><br><span class="line">        beta = np.ones(self.V)</span><br><span class="line"></span><br><span class="line">        self.params = &#123;</span><br><span class="line">            <span class="string">&#x27;gamma&#x27;</span>: gamma,</span><br><span class="line">            <span class="string">&#x27;rho&#x27;</span>: rho,</span><br><span class="line">            <span class="string">&#x27;eta&#x27;</span>: eta,</span><br><span class="line">            <span class="string">&#x27;alpha&#x27;</span>: alpha,</span><br><span class="line">            <span class="string">&#x27;beta&#x27;</span>: beta</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_gamma</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">         $$\gamma_&#123;mk&#125; = \alpha_k + \sum_&#123;n=1&#125;^&#123;N_m&#125; \eta_&#123;mnk&#125;$$</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        alpha = deepcopy(self.params[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">        eta = deepcopy(self.params[<span class="string">&#x27;eta&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预定义占位</span></span><br><span class="line">        gamma = np.zeros((self.M, self.K))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">            gamma[m] = alpha + np.<span class="built_in">sum</span>(eta[m], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化</span></span><br><span class="line">        gamma = np.array([gamma[:, k] / np.<span class="built_in">sum</span>(gamma, axis=<span class="number">1</span>) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K)]).T</span><br><span class="line">        <span class="keyword">return</span> gamma</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_rho</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        $$\rho_&#123;kv&#125; = \beta_v + \sum_&#123;m=1&#125;^M \sum_&#123;n=1&#125;^&#123;N_m&#125;  \eta_&#123;nmk&#125; w_&#123;mn&#125;^v $$</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        beta = deepcopy(self.params[<span class="string">&#x27;beta&#x27;</span>])</span><br><span class="line">        eta = deepcopy(self.params[<span class="string">&#x27;eta&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        rho = np.zeros((self.K, self.V))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K):</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(self.V):</span><br><span class="line">                sum_mn = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">                    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(self.N[m]):</span><br><span class="line">                        sum_mn += eta[m][n][k] * (self.texts[m][n] == self.tockens[v])</span><br><span class="line">                rho[k][v] = sum_mn + beta[v]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化</span></span><br><span class="line">        rho = np.array([rho[k] / np.<span class="built_in">sum</span>(rho[k]) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K)])</span><br><span class="line">        <span class="keyword">return</span> rho</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_eta</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        $$\eta_&#123;mnk&#125; \ltimes \exp(\Psi(\gamma_&#123;mk&#125;)</span></span><br><span class="line"><span class="string">        +\sum_&#123;v=1&#125;^V w_&#123;mn&#125;^v  \Psi(\rho_&#123;kv&#125;) - \Psi(\sum_&#123;s=1&#125;^V \rho_&#123;ks&#125;))$$</span></span><br><span class="line"><span class="string">        计算中需要归一化</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        gamma = deepcopy(self.params[<span class="string">&#x27;gamma&#x27;</span>])</span><br><span class="line">        rho = deepcopy(self.params[<span class="string">&#x27;rho&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        eta = np.array([np.ones((n, self.K)) <span class="keyword">for</span> n <span class="keyword">in</span> self.N])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(self.N[m]):</span><br><span class="line">                sum_k = <span class="number">0</span></span><br><span class="line">                eta_mn_k_list = np.zeros(self.K)</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.K):</span><br><span class="line">                    sum_digamma_rho_k = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(self.V):</span><br><span class="line">                        sum_digamma_rho_k += digamma(rho[k][v]) * (self.tockens[v] == self.texts[m][n])</span><br><span class="line">                    a = digamma(gamma[m][k]) + sum_digamma_rho_k + digamma(np.<span class="built_in">sum</span>(rho[k]))</span><br><span class="line">                    <span class="comment"># 为了防止指数增长中的内存溢出，限定一个阈值</span></span><br><span class="line">                    <span class="keyword">if</span> a &gt; <span class="number">20</span>:</span><br><span class="line">                        a = <span class="number">20</span></span><br><span class="line">                    sum_k += np.exp(a)</span><br><span class="line">                    eta_mn_k_list[k] = np.exp(a)</span><br><span class="line">                <span class="comment"># 将k维度上的数据归一化后赋值</span></span><br><span class="line">                eta[m][n] = eta_mn_k_list / sum_k</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> eta</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_E_step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新变分参数，为了方便理解，将gamma,rho, eta分别计算</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 更新gamma</span></span><br><span class="line">        gamma = self._update_gamma()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新rho</span></span><br><span class="line">        rho = self._update_rho()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新eta</span></span><br><span class="line">        eta = self._update_eta()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将结果写入</span></span><br><span class="line">        self.params[<span class="string">&#x27;gamma&#x27;</span>] = gamma</span><br><span class="line">        self.params[<span class="string">&#x27;rho&#x27;</span>] = rho</span><br><span class="line">        self.params[<span class="string">&#x27;eta&#x27;</span>] = eta</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_alpha</span>(<span class="params">self, max_iter=<span class="number">1000</span>, tol=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        $$\alpha_&#123;new&#125; = \alpha_&#123;old&#125; - H(\alpha_&#123;old&#125;)^&#123;-1&#125; g(\alpha_&#123;old&#125;)$$</span></span><br><span class="line"><span class="string">        其中g(alpha)是关于alpha的一阶导数</span></span><br><span class="line"><span class="string">        H(alpha)是关于alpha的Henssian矩阵</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        alpha = deepcopy(self.params[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">        gamma = deepcopy(self.params[<span class="string">&#x27;gamma&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            alpha_old = alpha</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算alpha的一阶导数</span></span><br><span class="line">            <span class="comment"># np.tile是将数据扩展</span></span><br><span class="line">            g = self.M * (digamma(np.<span class="built_in">sum</span>(alpha)) - digamma(alpha)) + \</span><br><span class="line">                np.<span class="built_in">sum</span>(</span><br><span class="line">                    digamma(gamma) - np.tile(digamma(np.<span class="built_in">sum</span>(gamma, axis=<span class="number">1</span>)), (self.K, <span class="number">1</span>)).T,</span><br><span class="line">                    axis=<span class="number">0</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算Hessen矩阵</span></span><br><span class="line">            h = -<span class="number">1</span> * self.M * polygamma(<span class="number">1</span>, alpha)</span><br><span class="line">            z = self.M * polygamma(<span class="number">1</span>, np.<span class="built_in">sum</span>(alpha))</span><br><span class="line">            c = np.<span class="built_in">sum</span>(g / h) / (z ** (-<span class="number">1.0</span>) + np.<span class="built_in">sum</span>(h ** (-<span class="number">1.0</span>)))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># update alpha</span></span><br><span class="line">            alpha = alpha - (g - c) / h</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 设置终止条件</span></span><br><span class="line">            <span class="keyword">if</span> np.sqrt(np.mean(np.square(alpha - alpha_old))) &lt; tol:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_beta</span>(<span class="params">self, max_iter=<span class="number">1000</span>, tol=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        $$\beta_&#123;new&#125; = \beta_&#123;old&#125; - H(\beta_&#123;old&#125;)^&#123;-1&#125;g(\beta_&#123;old&#125;)$$</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        beta = deepcopy(self.params[<span class="string">&#x27;beta&#x27;</span>])</span><br><span class="line">        rho = deepcopy(self.params[<span class="string">&#x27;rho&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            beta_old = beta</span><br><span class="line"></span><br><span class="line">            g = self.K * (digamma(np.<span class="built_in">sum</span>(beta)) - digamma(beta)) + \</span><br><span class="line">                np.<span class="built_in">sum</span>(</span><br><span class="line">                    digamma(rho) - np.tile(digamma(np.<span class="built_in">sum</span>(rho, axis=<span class="number">1</span>)), (self.V, <span class="number">1</span>)).T,</span><br><span class="line">                    axis=<span class="number">0</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            h = -<span class="number">1</span> * self.K * polygamma(<span class="number">1</span>, beta)</span><br><span class="line">            z = self.K * polygamma(<span class="number">1</span>, np.<span class="built_in">sum</span>(beta))</span><br><span class="line">            c = np.<span class="built_in">sum</span>(g / h) / (z ** (-<span class="number">1.0</span>) + np.<span class="built_in">sum</span>(h ** (-<span class="number">1.0</span>)))</span><br><span class="line"></span><br><span class="line">            beta = beta - (g - c) / h</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> np.sqrt(np.mean(np.square(beta - beta_old))) &lt; tol:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> beta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_M_step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新模型参数，alpha, beta</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 更新alpha</span></span><br><span class="line">        alpha = self._update_alpha()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新beta</span></span><br><span class="line">        beta = self._update_beta()</span><br><span class="line"></span><br><span class="line">        self.params[<span class="string">&#x27;alpha&#x27;</span>] = alpha</span><br><span class="line">        self.params[<span class="string">&#x27;beta&#x27;</span>] = beta</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, texts, tokens, max_iter=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练入口</span></span><br><span class="line"><span class="string">        :param texts:</span></span><br><span class="line"><span class="string">        :param tokens:</span></span><br><span class="line"><span class="string">        :param max_iter:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.M = <span class="built_in">len</span>(texts)</span><br><span class="line">        self.tockens = tokens</span><br><span class="line">        self.V = <span class="built_in">len</span>(tokens)</span><br><span class="line">        self.N = np.array([<span class="built_in">len</span>(d) <span class="keyword">for</span> d <span class="keyword">in</span> texts])</span><br><span class="line"></span><br><span class="line">        self.texts = texts</span><br><span class="line"></span><br><span class="line">        self._init_params()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            print(<span class="string">&#x27;iter: &#x27;</span>, i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            self._E_step()</span><br><span class="line">            self._M_step()</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/03/29/再谈概率潜在语义分析模型/" target="_blank" title="再谈概率潜在语义分析模型">http://example.com/2021/03/29/再谈概率潜在语义分析模型/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2021/03/27/%E5%86%8D%E8%B0%88%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">再谈潜在语义分析</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">主题模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90"><span class="nav-number">2.</span> <span class="nav-text">概率潜在语义分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">生成模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E7%8E%B0%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">共现模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BD%9C%E5%9C%A8%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83"><span class="nav-number">3.</span> <span class="nav-text">潜在狄利克雷分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%AE%9A%E4%B9%89"><span class="nav-number">3.1.</span> <span class="nav-text">分布定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">LDA模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA%E7%9A%84%E5%90%89%E5%B8%83%E6%96%AF%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">LDA的吉布斯抽样算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA%E7%9A%84%E5%8F%98%E5%88%86EM%E6%B3%95"><span class="nav-number">3.4.</span> <span class="nav-text">LDA的变分EM法</span></a></li></ol></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright © 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>