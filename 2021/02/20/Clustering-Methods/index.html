<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>clustering methods | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="unsupervised learning" />
  
  
  
  
  <meta name="description" content="Fundamental ConceptsBefore we talk about clustering methods, there are some required knowledge. Similarity or Distance1.Minkowski distance:$$d_{ig} &#x3D; (\sum_{k&#x3D;1}^{m}|x_{ki}-x_{kj}|^p)^{\frac{1}{p}}$$w">
<meta property="og:type" content="article">
<meta property="og:title" content="Clustering Methods">
<meta property="og:url" content="http://example.com/2021/02/20/Clustering-Methods/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="Fundamental ConceptsBefore we talk about clustering methods, there are some required knowledge. Similarity or Distance1.Minkowski distance:$$d_{ig} &#x3D; (\sum_{k&#x3D;1}^{m}|x_{ki}-x_{kj}|^p)^{\frac{1}{p}}$$w">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-02-20T08:23:47.000Z">
<meta property="article:modified_time" content="2021-02-21T11:03:28.027Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="unsupervised learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Clustering-Methods" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Clustering Methods
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/02/20/Clustering-Methods/" class="article-date">
	  <time datetime="2021-02-20T08:23:47.000Z" itemprop="datePublished">2021-02-20</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Fundamental-Concepts"><a href="#Fundamental-Concepts" class="headerlink" title="Fundamental Concepts"></a>Fundamental Concepts</h2><p>Before we talk about clustering methods, there are some required knowledge.</p>
<h3 id="Similarity-or-Distance"><a href="#Similarity-or-Distance" class="headerlink" title="Similarity or Distance"></a>Similarity or Distance</h3><p>1.Minkowski distance:<br>$$<br>d_{ig} = (\sum_{k=1}^{m}|x_{ki}-x_{kj}|^p)^{\frac{1}{p}}<br>$$<br>where p‚â•1, and if p=2, it is Euclidean distance:<br>$$<br>d_{ij}=(\sum_{k=1}^{m}|x_{ki}-x_{kj}|^2)^{\frac{1}{2}}<br>$$<br> when p=1, it is Manhattan distance:<br>$$<br>d_{ij}=\sum_{k=1}^m|x_{ki}-x_{kj}|<br>$$<br>when p=‚àû, it is Chebyshev distance:<br>$$<br>d_{ij}=\mathop{max}\limits_{k} |x_{ki}-x_{kj}|<br>$$<br>2.Mahalanobis distance:<br>$$<br>d_{ij}=[(x_i-x_j)^TS^{-1}(x_i-x_j)]^{\frac{1}{2}}<br>$$<br>where S is the covariance matrix of X. Mahalanobis distance is unitless and scale-invariant.</p>
<p>3.Correlation coeffiicient:<br>$$<br>r_{ij}=\frac{\sum_{k=1}^m(x_{ki}-\overline{x}<em>i)(x</em>{kj}-\overline{x}<em>j)}{[\sum</em>{k=1}^m (x_{ki}-\overline{x}<em>i)^2\sum</em>{k=1}^m(x_{kj}-\overline{x}_j)^2]^{\frac{1}{2}}}<br>$$<br>where \overline{x}_i and \overline{x}_j are mean values of x_i and x_j.</p>
<p>4.cosine:<br>$$<br>s_{ij}=\frac{\sum_{k=1}^m x_{ki}x_{kj}}{[\sum_{k=1}^{m}x_{ki}^2 \sum_{k=1}^mx_{kj}^2]^{\frac{1}{2}}}<br>$$</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>There are kinds of definitions of cluster  as follows :</p>
<p>1.G is a cluster for any  x<sub>i</sub> and x<sub>j</sub> have:<br>$$<br>d_{ij}‚â§T<br>$$<br>where T is a positive number.</p>
<p>2.x<sub>i</sub> is in G cluster, if for any j in G cluster:<br>$$<br>\frac{1}{n_G-1}\sum_{x_j\in G}d_{ij}‚â§T<br>$$<br>where T is a positive number.</p>
<p>3.G is a cluster if x<sub>i</sub> and x<sub>j</sub> :<br>$$<br>\frac{1}{n_G(n_G-1)}\sum_{x_i\in G}\sum_{x_j\in G}d_{ij}‚â§T<br>$$<br>The features of cluster are as follows:</p>
<p>1.mean value(centroid):<br>$$<br>\overline{x}<em>G=\frac{1}{n_G}\sum^{n_G}</em>{i=1}x_i<br>$$<br>2.diameter:<br>$$<br>D_G=\mathop{max}\limits_{x_i,x_j\in G}d_{ij}<br>$$<br>3.scatter matrix A<sub>G</sub> and covariance matrix S<sub>G</sub>:<br>$$<br>A_G=\sum_{i=1}^{n_G}(x_i-\overline{x}<em>G)(x_i-\overline{x}_G)^T\<br>S_G=\frac{1}{m-1}A_G\<br>=\frac{1}{m-1}\sum</em>{i=1}^{n_G}(x_i-\overline{x}_G)(x_i-\overline{x}_G)^T<br>$$<br>where m is the dimensionalities of x<sub>k</sub>.</p>
<h3 id="Distance-linkage-between-clusters"><a href="#Distance-linkage-between-clusters" class="headerlink" title="Distance(linkage) between clusters"></a>Distance(linkage) between clusters</h3><p>1.single linkage:<br>$$<br>D_{pq}=min{d_{ij}|x_i\in G_p,x_j\in G_q}<br>$$<br>2.complete linkage:<br>$$<br>D_{pq}=max{d_{ij}|x_i\in G_p,x_j\in G_q}<br>$$<br>3.centroid linkage:<br>$$<br>D_{pq}=d_{\overline{x}<em>p \overline{x}<em>q}<br>$$<br>4.average linkage:<br>$$<br>D</em>{pq}=\frac{1}{n_p n_q}\sum</em>{x_i\in G_q}\sum_{x_j \in G_p}d_{ij}<br>$$</p>
<h2 id="Hierarchical-clustering"><a href="#Hierarchical-clustering" class="headerlink" title="Hierarchical clustering"></a>Hierarchical clustering</h2><p>Hierarchical clustering is separating data into groups based on some measure of similarity, finding a way to measure how they‚Äôre alike and different, and further narrowing down the data.</p>
<p>Hierarchical clustering is divided into agglomerative and divisive. Divisive clustering is known as the top-down approach. We take a large cluster and start dividing it into two, three, four, or more clusters. Agglomerative clustering is known as a bottom-up approach. Consider it as bringing things together.</p>
<p>There are three elements we need to know before cluster:(1)similarity;(2)merge rules;(3)stop condition.</p>
<p>Agglomerative hierarchical clustering:</p>
<p>Input:a sample set consist of n samples and the distance between samples;</p>
<p>Output:a hierarchical cluster for the sample set.</p>
<p>step 1: calculate the Euclidean distance between each other for n samples, represent it as D={d<sub>ij</sub>}<sub>n*n</sub>;</p>
<p>step 2: construct n clusters, each cluster only contains one sample;</p>
<p>step 3:merge two clusters whose distance is the shortest between clusters to construct a new cluster;</p>
<p>step 4:calculate the distance between new cluster and other clusters, stop clustering if the number of clusters is 1, otherwise jump to step3.</p>
<p>The time complexity of the algorithm is O(n<sup>3</sup>m), m is the dimensionalities of sample and n is the nubmer of samples.</p>
<h2 id="K-means-clustering"><a href="#K-means-clustering" class="headerlink" title="K-means clustering"></a>K-means clustering</h2><p>K-Means clustering algorithm is defined as a unsupervised learning methods having an iterative process in which the dataset are grouped into k number of predefined non-overlapping clusters or subgroups making the inner points of the cluster as similar as possible while trying to keep the clusters at distinct space it allocates the data points to a cluster so that the sum of the squared distance between the clusters centroid and the data point is at a minimum, at this position the centroid of the cluster is the arithmetic mean of the data points that are in the clusters.</p>
<p>k-means clustering:</p>
<p>Input:n samples set X;</p>
<p>Output: cluster set C;</p>
<p>step 1:initialization, make t = 0, select k samples as initial centroids randomly, m<sup>(0)</sup>=(m<sub>1</sub><sup>(0)</sup>,‚Ä¶,m<sub>l</sub><sup>(0)</sup>,‚Ä¶,m<sub>k</sub><sup>(0)</sup>)</p>
<p>step 2:cluster for samples, for centroids m<sup>(t)</sup>, m<sub>l</sub><sup>(t)</sup> is the centroid of cluster G<sub>l</sub>, calculate each the distance of sample to its cluster cnetroid, assign each sample into their nearest cluster, we can get a new cluster C<sup>(t)</sup>.</p>
<p>step 3:calculate the new cluster centroid, for C<sup>(t)</sup>, calculate the average value of each cluster as new centroids m<sup>(t+1)</sup>=(m<sub>1</sub><sup>(t+1)</sup>,‚Ä¶,m<sub>l</sub><sup>(t+1)</sup>,‚Ä¶,m<sub>k</sub><sup>(t+1)</sup>).</p>
<p>step 4:if the literation converges or elegible for termination, output C<sup>*</sup>=C<sup>(t)</sup>. Otherwise, t+=1, jump to step 2. The time complexity of k-means clustering is O(mnk). Where m is the dimensionality of sample, n is the number of samples, k is the number of clusters.</p>
<h2 id="EM-GMM"><a href="#EM-GMM" class="headerlink" title="EM-GMM"></a>EM-GMM</h2><p>In most cases, Expectation-Maximization is an effective method to train Gaussian mixture model</p>
<p>EM algorithm:</p>
<p>Input:observed data Y, unobserved data Z, statistical model P(Y,Z|Œ∏) and P(Z|Y,Œ∏)</p>
<p>Output:model parameters Œ∏.</p>
<p>step 1:choose initial value Œ∏<sup>(0)</sup>, start the iteration;</p>
<p>step 2:expectation step, notate Œ∏<sup>(i)</sup> as the estimated value of parameter Œ∏ in i iteration, in i+1 iteration, calculate<br>$$<br>Q(\theta,\theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]\=\sum_{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)})<br>$$<br>where P(Z|Y,Œ∏<sup>(i)</sup>) represents a conditional probability distribution of given observed data Y and hidden variables Z under estimated parameter Œ∏<sup>(i)</sup>;</p>
<p>step 3:maximazation step, find the parameters that maximize the expectation of the Q function:<br>$$<br>\theta^{(i+1)}=arg\mathop{max}\limits_{\theta}Q(\theta,\theta^{(i)})<br>$$<br>step 4:repeat step 2 and step 3, until convergence.</p>
<p>Q function is the expectation of logP(Y,Z|Œ∏) under P(Z|Y,Œ∏<sup>(i)</sup>).</p>
<p>The termination always be:<br>$$<br>||\theta^{(i+1)}-\theta^{(i)}||&lt;\epsilon_1\ or\ ||Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})||<br>$$<br>Gaussian mixture model is a probabilitisc distribution model:<br>$$<br>P(y|\theta)=\sum_{k=1}^{K}\alpha_k\phi(y|\theta_k)<br>$$<br>where ùõÇ<sub>k</sub>‚â•0, the sum of ùõÇ<sub>k</sub> euqals to 1, …∏(y|Œ∏<sub>k</sub>)  is the density of Gaussian distribution, Œ∏<sub>k</sub>=(Œº<sub>k</sub>,ùûÇ<sub>k</sub>)</p>
<p>We use EM algorithm to estimate parameter Œ∏={ùõÇ<sub>1</sub>,‚Ä¶,ùõÇ<sub>k</sub>,ùõâ<sub>1</sub>,‚Ä¶,ùõâ<sub>k</sub>};</p>
<p>The configuration of hidden latent variable ùû¨ is omitted, you can read <em>ÊùéËà™. ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï. Ê∏ÖÂçéÂ§ßÂ≠¶Âá∫ÁâàÁ§æ, 2012.</em> for details.</p>
<p>EM-GMM clustering:</p>
<p>Input:ovserved data Y={y<sub>1</sub>,y<sub>2</sub>,‚Ä¶,y<sub>k</sub>}, Gaussian mixture model</p>
<p>Output:parameters of Gaussian mixture model.</p>
<p>step 1:initialize the parameters;</p>
<p>step 2:according to the current parameters, calculate the responsibility of sub model  to observed data y<sub>j</sub>.<br>$$<br>\hat{\gamma}_{jk}=\frac{\alpha_k \phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k \phi(y_j|\theta_k)}<br>$$</p>
<p>step 3:calculate the model parameters for next iteration:<br>$$<br>\hat{\mu}<em>k=\frac{\sum</em>{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}}\\hat{\sigma}<em>k^2=\frac{\sum</em>{j=1}^{N}\hat\gamma_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^{N}\hat\gamma_{jk}}\\hat\alpha_{k}=\frac{\sum_{j=1}^N\hat\gamma_{jk}}{N}<br>$$<br>step 4:repeat step 2 and step 3, until convergence.</p>
<p>More clustering methods can be viewed in papers as follows:</p>
<p><strong>[1]Pelleg D . Extending K-means with efficient estimation of the number of clusters[J]. ICML2000, 2000, 17.</strong></p>
<p><strong>[2]Comaniciu D , Meer P . Mean shift: a robust approach toward feature space analysis[J]. IEEE Trans Pattern Analysis &amp; Machine Intelligence, 2002, 24(5):603-619.</strong></p>
<p><strong>[3]Shi J , Malik J M . Normalized Cuts and Image Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000.</strong></p>
<p><strong>[4]Ester M . A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise[J]. Proc.int.conf.knowledg Discovery &amp; Data Mining, 1996.</strong></p>
<p><strong>[5]Dhillon, I. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD) (pp. 269 ‚Äì 274). New York: ACM Press, 2001.</strong></p>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/02/20/Clustering-Methods/" target="_blank" title="Clustering Methods">http://example.com/2021/02/20/Clustering-Methods/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- Êù•ÂøÖÂäõCityÁâàÂÆâË£Ö‰ª£Á†Å -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>‰∏∫Ê≠£Â∏∏‰ΩøÁî®Êù•ÂøÖÂäõËØÑËÆ∫ÂäüËÉΩËØ∑ÊøÄÊ¥ªJavaScript</noscript>
		</div>
		<!-- CityÁâàÂÆâË£Ö‰ª£Á†ÅÂ∑≤ÂÆåÊàê -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/02/21/Singular-Value-Decomposition/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Singular Value Decomposition
        
      </div>
    </a>
  
  
    <a href="/2021/02/19/EEG-data-analysis-with-MNE-Python/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">EEG data analysis with MNE-Python</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Fundamental-Concepts"><span class="nav-number">1.</span> <span class="nav-text">Fundamental Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Similarity-or-Distance"><span class="nav-number">1.1.</span> <span class="nav-text">Similarity or Distance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster"><span class="nav-number">1.2.</span> <span class="nav-text">Cluster</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distance-linkage-between-clusters"><span class="nav-number">1.3.</span> <span class="nav-text">Distance(linkage) between clusters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hierarchical-clustering"><span class="nav-number">2.</span> <span class="nav-text">Hierarchical clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-means-clustering"><span class="nav-number">3.</span> <span class="nav-text">K-means clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EM-GMM"><span class="nav-number">4.</span> <span class="nav-text">EM-GMM</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">ËÆæÁΩÆ</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              Ê≠£ÊñáÂ≠óÂè∑Â§ßÂ∞è
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            ÊÇ®Â∑≤Ë∞ÉÊï¥È°µÈù¢Â≠ó‰ΩìÂ§ßÂ∞è
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              Â§úÈó¥Êä§ÁúºÊ®°Âºè
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            Â§úÈó¥Ê®°ÂºèÂ∑≤ÁªèÂºÄÂêØÔºåÂÜçÊ¨°ÂçïÂáªÊåâÈíÆÂç≥ÂèØÂÖ≥Èó≠ 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ÂÖ≥ ‰∫é&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright ¬© 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">√ó</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>