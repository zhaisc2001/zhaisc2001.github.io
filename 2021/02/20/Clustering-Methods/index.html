<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>clustering methods | Sisicca</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="unsupervised learning" />
  
  
  
  
  <meta name="description" content="Fundamental ConceptsBefore we talk about clustering methods, there are some required knowledge. Similarity or Distance1.Minkowski distance:$$d_{ig} &#x3D; (\sum_{k&#x3D;1}^{m}|x_{ki}-x_{kj}|^p)^{\frac{1}{p}}$$w">
<meta property="og:type" content="article">
<meta property="og:title" content="Clustering Methods">
<meta property="og:url" content="http://example.com/2021/02/20/Clustering-Methods/index.html">
<meta property="og:site_name" content="Sisicca">
<meta property="og:description" content="Fundamental ConceptsBefore we talk about clustering methods, there are some required knowledge. Similarity or Distance1.Minkowski distance:$$d_{ig} &#x3D; (\sum_{k&#x3D;1}^{m}|x_{ki}-x_{kj}|^p)^{\frac{1}{p}}$$w">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-02-20T08:23:47.000Z">
<meta property="article:modified_time" content="2021-02-21T11:03:28.027Z">
<meta property="article:author" content="Uestc_Sicca">
<meta property="article:tag" content="unsupervised learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Sisicca" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.3.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Clustering-Methods" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Clustering Methods
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/02/20/Clustering-Methods/" class="article-date">
	  <time datetime="2021-02-20T08:23:47.000Z" itemprop="datePublished">2021-02-20</time>
	</a>

      
    <a class="article-category-link" href="/categories/technology/">technology</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Fundamental-Concepts"><a href="#Fundamental-Concepts" class="headerlink" title="Fundamental Concepts"></a>Fundamental Concepts</h2><p>Before we talk about clustering methods, there are some required knowledge.</p>
<h3 id="Similarity-or-Distance"><a href="#Similarity-or-Distance" class="headerlink" title="Similarity or Distance"></a>Similarity or Distance</h3><p>1.Minkowski distance:<br>$$<br>d_{ig} = (\sum_{k=1}^{m}|x_{ki}-x_{kj}|^p)^{\frac{1}{p}}<br>$$<br>where p≥1, and if p=2, it is Euclidean distance:<br>$$<br>d_{ij}=(\sum_{k=1}^{m}|x_{ki}-x_{kj}|^2)^{\frac{1}{2}}<br>$$<br> when p=1, it is Manhattan distance:<br>$$<br>d_{ij}=\sum_{k=1}^m|x_{ki}-x_{kj}|<br>$$<br>when p=∞, it is Chebyshev distance:<br>$$<br>d_{ij}=\mathop{max}\limits_{k} |x_{ki}-x_{kj}|<br>$$<br>2.Mahalanobis distance:<br>$$<br>d_{ij}=[(x_i-x_j)^TS^{-1}(x_i-x_j)]^{\frac{1}{2}}<br>$$<br>where S is the covariance matrix of X. Mahalanobis distance is unitless and scale-invariant.</p>
<p>3.Correlation coeffiicient:<br>$$<br>r_{ij}=\frac{\sum_{k=1}^m(x_{ki}-\overline{x}<em>i)(x</em>{kj}-\overline{x}<em>j)}{[\sum</em>{k=1}^m (x_{ki}-\overline{x}<em>i)^2\sum</em>{k=1}^m(x_{kj}-\overline{x}_j)^2]^{\frac{1}{2}}}<br>$$<br>where \overline{x}_i and \overline{x}_j are mean values of x_i and x_j.</p>
<p>4.cosine:<br>$$<br>s_{ij}=\frac{\sum_{k=1}^m x_{ki}x_{kj}}{[\sum_{k=1}^{m}x_{ki}^2 \sum_{k=1}^mx_{kj}^2]^{\frac{1}{2}}}<br>$$</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>There are kinds of definitions of cluster  as follows :</p>
<p>1.G is a cluster for any  x<sub>i</sub> and x<sub>j</sub> have:<br>$$<br>d_{ij}≤T<br>$$<br>where T is a positive number.</p>
<p>2.x<sub>i</sub> is in G cluster, if for any j in G cluster:<br>$$<br>\frac{1}{n_G-1}\sum_{x_j\in G}d_{ij}≤T<br>$$<br>where T is a positive number.</p>
<p>3.G is a cluster if x<sub>i</sub> and x<sub>j</sub> :<br>$$<br>\frac{1}{n_G(n_G-1)}\sum_{x_i\in G}\sum_{x_j\in G}d_{ij}≤T<br>$$<br>The features of cluster are as follows:</p>
<p>1.mean value(centroid):<br>$$<br>\overline{x}<em>G=\frac{1}{n_G}\sum^{n_G}</em>{i=1}x_i<br>$$<br>2.diameter:<br>$$<br>D_G=\mathop{max}\limits_{x_i,x_j\in G}d_{ij}<br>$$<br>3.scatter matrix A<sub>G</sub> and covariance matrix S<sub>G</sub>:<br>$$<br>A_G=\sum_{i=1}^{n_G}(x_i-\overline{x}<em>G)(x_i-\overline{x}_G)^T\<br>S_G=\frac{1}{m-1}A_G\<br>=\frac{1}{m-1}\sum</em>{i=1}^{n_G}(x_i-\overline{x}_G)(x_i-\overline{x}_G)^T<br>$$<br>where m is the dimensionalities of x<sub>k</sub>.</p>
<h3 id="Distance-linkage-between-clusters"><a href="#Distance-linkage-between-clusters" class="headerlink" title="Distance(linkage) between clusters"></a>Distance(linkage) between clusters</h3><p>1.single linkage:<br>$$<br>D_{pq}=min{d_{ij}|x_i\in G_p,x_j\in G_q}<br>$$<br>2.complete linkage:<br>$$<br>D_{pq}=max{d_{ij}|x_i\in G_p,x_j\in G_q}<br>$$<br>3.centroid linkage:<br>$$<br>D_{pq}=d_{\overline{x}<em>p \overline{x}<em>q}<br>$$<br>4.average linkage:<br>$$<br>D</em>{pq}=\frac{1}{n_p n_q}\sum</em>{x_i\in G_q}\sum_{x_j \in G_p}d_{ij}<br>$$</p>
<h2 id="Hierarchical-clustering"><a href="#Hierarchical-clustering" class="headerlink" title="Hierarchical clustering"></a>Hierarchical clustering</h2><p>Hierarchical clustering is separating data into groups based on some measure of similarity, finding a way to measure how they’re alike and different, and further narrowing down the data.</p>
<p>Hierarchical clustering is divided into agglomerative and divisive. Divisive clustering is known as the top-down approach. We take a large cluster and start dividing it into two, three, four, or more clusters. Agglomerative clustering is known as a bottom-up approach. Consider it as bringing things together.</p>
<p>There are three elements we need to know before cluster:(1)similarity;(2)merge rules;(3)stop condition.</p>
<p>Agglomerative hierarchical clustering:</p>
<p>Input:a sample set consist of n samples and the distance between samples;</p>
<p>Output:a hierarchical cluster for the sample set.</p>
<p>step 1: calculate the Euclidean distance between each other for n samples, represent it as D={d<sub>ij</sub>}<sub>n*n</sub>;</p>
<p>step 2: construct n clusters, each cluster only contains one sample;</p>
<p>step 3:merge two clusters whose distance is the shortest between clusters to construct a new cluster;</p>
<p>step 4:calculate the distance between new cluster and other clusters, stop clustering if the number of clusters is 1, otherwise jump to step3.</p>
<p>The time complexity of the algorithm is O(n<sup>3</sup>m), m is the dimensionalities of sample and n is the nubmer of samples.</p>
<h2 id="K-means-clustering"><a href="#K-means-clustering" class="headerlink" title="K-means clustering"></a>K-means clustering</h2><p>K-Means clustering algorithm is defined as a unsupervised learning methods having an iterative process in which the dataset are grouped into k number of predefined non-overlapping clusters or subgroups making the inner points of the cluster as similar as possible while trying to keep the clusters at distinct space it allocates the data points to a cluster so that the sum of the squared distance between the clusters centroid and the data point is at a minimum, at this position the centroid of the cluster is the arithmetic mean of the data points that are in the clusters.</p>
<p>k-means clustering:</p>
<p>Input:n samples set X;</p>
<p>Output: cluster set C;</p>
<p>step 1:initialization, make t = 0, select k samples as initial centroids randomly, m<sup>(0)</sup>=(m<sub>1</sub><sup>(0)</sup>,…,m<sub>l</sub><sup>(0)</sup>,…,m<sub>k</sub><sup>(0)</sup>)</p>
<p>step 2:cluster for samples, for centroids m<sup>(t)</sup>, m<sub>l</sub><sup>(t)</sup> is the centroid of cluster G<sub>l</sub>, calculate each the distance of sample to its cluster cnetroid, assign each sample into their nearest cluster, we can get a new cluster C<sup>(t)</sup>.</p>
<p>step 3:calculate the new cluster centroid, for C<sup>(t)</sup>, calculate the average value of each cluster as new centroids m<sup>(t+1)</sup>=(m<sub>1</sub><sup>(t+1)</sup>,…,m<sub>l</sub><sup>(t+1)</sup>,…,m<sub>k</sub><sup>(t+1)</sup>).</p>
<p>step 4:if the literation converges or elegible for termination, output C<sup>*</sup>=C<sup>(t)</sup>. Otherwise, t+=1, jump to step 2. The time complexity of k-means clustering is O(mnk). Where m is the dimensionality of sample, n is the number of samples, k is the number of clusters.</p>
<h2 id="EM-GMM"><a href="#EM-GMM" class="headerlink" title="EM-GMM"></a>EM-GMM</h2><p>In most cases, Expectation-Maximization is an effective method to train Gaussian mixture model</p>
<p>EM algorithm:</p>
<p>Input:observed data Y, unobserved data Z, statistical model P(Y,Z|θ) and P(Z|Y,θ)</p>
<p>Output:model parameters θ.</p>
<p>step 1:choose initial value θ<sup>(0)</sup>, start the iteration;</p>
<p>step 2:expectation step, notate θ<sup>(i)</sup> as the estimated value of parameter θ in i iteration, in i+1 iteration, calculate<br>$$<br>Q(\theta,\theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]\=\sum_{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)})<br>$$<br>where P(Z|Y,θ<sup>(i)</sup>) represents a conditional probability distribution of given observed data Y and hidden variables Z under estimated parameter θ<sup>(i)</sup>;</p>
<p>step 3:maximazation step, find the parameters that maximize the expectation of the Q function:<br>$$<br>\theta^{(i+1)}=arg\mathop{max}\limits_{\theta}Q(\theta,\theta^{(i)})<br>$$<br>step 4:repeat step 2 and step 3, until convergence.</p>
<p>Q function is the expectation of logP(Y,Z|θ) under P(Z|Y,θ<sup>(i)</sup>).</p>
<p>The termination always be:<br>$$<br>||\theta^{(i+1)}-\theta^{(i)}||&lt;\epsilon_1\ or\ ||Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})||<br>$$<br>Gaussian mixture model is a probabilitisc distribution model:<br>$$<br>P(y|\theta)=\sum_{k=1}^{K}\alpha_k\phi(y|\theta_k)<br>$$<br>where 𝛂<sub>k</sub>≥0, the sum of 𝛂<sub>k</sub> euqals to 1, ɸ(y|θ<sub>k</sub>)  is the density of Gaussian distribution, θ<sub>k</sub>=(μ<sub>k</sub>,𝞂<sub>k</sub>)</p>
<p>We use EM algorithm to estimate parameter θ={𝛂<sub>1</sub>,…,𝛂<sub>k</sub>,𝛉<sub>1</sub>,…,𝛉<sub>k</sub>};</p>
<p>The configuration of hidden latent variable 𝞬 is omitted, you can read <em>李航. 统计学习方法. 清华大学出版社, 2012.</em> for details.</p>
<p>EM-GMM clustering:</p>
<p>Input:ovserved data Y={y<sub>1</sub>,y<sub>2</sub>,…,y<sub>k</sub>}, Gaussian mixture model</p>
<p>Output:parameters of Gaussian mixture model.</p>
<p>step 1:initialize the parameters;</p>
<p>step 2:according to the current parameters, calculate the responsibility of sub model  to observed data y<sub>j</sub>.<br>$$<br>\hat{\gamma}_{jk}=\frac{\alpha_k \phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k \phi(y_j|\theta_k)}<br>$$</p>
<p>step 3:calculate the model parameters for next iteration:<br>$$<br>\hat{\mu}<em>k=\frac{\sum</em>{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}}\\hat{\sigma}<em>k^2=\frac{\sum</em>{j=1}^{N}\hat\gamma_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^{N}\hat\gamma_{jk}}\\hat\alpha_{k}=\frac{\sum_{j=1}^N\hat\gamma_{jk}}{N}<br>$$<br>step 4:repeat step 2 and step 3, until convergence.</p>
<p>More clustering methods can be viewed in papers as follows:</p>
<p><strong>[1]Pelleg D . Extending K-means with efficient estimation of the number of clusters[J]. ICML2000, 2000, 17.</strong></p>
<p><strong>[2]Comaniciu D , Meer P . Mean shift: a robust approach toward feature space analysis[J]. IEEE Trans Pattern Analysis &amp; Machine Intelligence, 2002, 24(5):603-619.</strong></p>
<p><strong>[3]Shi J , Malik J M . Normalized Cuts and Image Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000.</strong></p>
<p><strong>[4]Ester M . A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise[J]. Proc.int.conf.knowledg Discovery &amp; Data Mining, 1996.</strong></p>
<p><strong>[5]Dhillon, I. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD) (pp. 269 – 274). New York: ACM Press, 2001.</strong></p>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Uestc_Sicca</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2021/02/20/Clustering-Methods/" target="_blank" title="Clustering Methods">http://example.com/2021/02/20/Clustering-Methods/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/02/21/Singular-Value-Decomposition/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Singular Value Decomposition
        
      </div>
    </a>
  
  
    <a href="/2021/02/19/EEG-data-analysis-with-MNE-Python/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">EEG data analysis with MNE-Python</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Fundamental-Concepts"><span class="nav-number">1.</span> <span class="nav-text">Fundamental Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Similarity-or-Distance"><span class="nav-number">1.1.</span> <span class="nav-text">Similarity or Distance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster"><span class="nav-number">1.2.</span> <span class="nav-text">Cluster</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distance-linkage-between-clusters"><span class="nav-number">1.3.</span> <span class="nav-text">Distance(linkage) between clusters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hierarchical-clustering"><span class="nav-number">2.</span> <span class="nav-text">Hierarchical clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-means-clustering"><span class="nav-number">3.</span> <span class="nav-text">K-means clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EM-GMM"><span class="nav-number">4.</span> <span class="nav-text">EM-GMM</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2020 - 2021 Sisicca All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Sisicca
          </div>
          <div class="panel-body">
            Copyright © 2021 Uestc_Sicca All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>